{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/mufaddalhamidofficial/tensorflow_course/main/helper_funcs.py\n",
    "!wget https://media.githubusercontent.com/media/mufaddalhamidofficial/skimlit_ai/main/data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from helper_funcs import create_tensorboard_callback, create_checkpoint_callback, plot_loss_curves, compare_historys, unzip_data, calculate_results\n",
    "import tensorflow_hub as hub\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "unzip_data(\"data.zip\", '/kaggle/temp/')\n",
    "!rm -rf data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_NAME_01 = \"/kaggle/temp/data/01_percent\"\n",
    "DIR_NAME_1 = \"/kaggle/temp/data/1_percent\"\n",
    "DIR_NAME_10 = \"/kaggle/temp/data/10_percent\"\n",
    "DIR_NAME_100 = \"/kaggle/temp/data/100_percent\"\n",
    "\n",
    "test_df_100 = pd.read_csv(DIR_NAME_100 + \"/test.csv\")\n",
    "test_df_100 = test_df_100.iloc[:, 1:]\n",
    "\n",
    "test_sentences_ = test_df_100.text.to_numpy()\n",
    "test_labels_ = test_df_100.target.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(experiment_name, use_early_stopping = True, patience=10, use_tensorboard = True, use_model_checkpoint = True):\n",
    "    callbacks = [\n",
    "    ]\n",
    "    if use_tensorboard:\n",
    "        callbacks.append(\n",
    "            create_tensorboard_callback(\n",
    "                dir_name=\"skim_lit/tensorboard\",\n",
    "                experiment_name=experiment_name,\n",
    "            )\n",
    "        )\n",
    "    if use_model_checkpoint:\n",
    "        callbacks.append(\n",
    "            create_checkpoint_callback(\n",
    "                dir_name=\"skim_lit/checkpoint\",\n",
    "                experiment_name=experiment_name,\n",
    "                monitor=\"val_accuracy\",\n",
    "            )\n",
    "        )\n",
    "    if use_early_stopping:\n",
    "        \n",
    "        callbacks.append(tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_accuracy\",\n",
    "            patience=patience,\n",
    "            restore_best_weights=True,\n",
    "        ))\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model s Data Preps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get and Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_1 = pd.read_csv(DIR_NAME_1 + \"/train.csv\")\n",
    "train_sentences = train_data_1.text.to_numpy()\n",
    "train_labels = train_data_1.target.to_numpy()\n",
    "\n",
    "val_data_1 = pd.read_csv(DIR_NAME_1 + \"/val.csv\")\n",
    "val_sentences = val_data_1.text.to_numpy()\n",
    "val_labels = val_data_1.target.to_numpy()\n",
    "\n",
    "test_sentences = test_sentences_.copy()\n",
    "test_labels = test_labels_.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "train_labels_one_hot = one_hot_encoder.fit_transform(train_labels.reshape(-1, 1))\n",
    "val_labels_one_hot = one_hot_encoder.transform(val_labels.reshape(-1, 1))\n",
    "test_labels_one_hot = one_hot_encoder.transform(test_labels.reshape(-1, 1))\n",
    "\n",
    "train_labels_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = list(one_hot_encoder.categories_[0])\n",
    "classes_count = len(class_names) # type: ignore\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "train_chars = [\" \".join(list(sentence)) for sentence in train_sentences]\n",
    "val_chars = [\" \".join(list(sentence)) for sentence in val_sentences]\n",
    "test_chars = [\" \".join(list(sentence)) for sentence in test_sentences]\n",
    "\n",
    "char_lens = [len(sentence.split(' ')) for sentence in train_chars]\n",
    "\n",
    "seq_char_len = int(np.percentile(char_lens, 90))\n",
    "\n",
    "alphabet = string.ascii_lowercase + string.digits + string.punctuation\n",
    "\n",
    "NUM_CHAR_TOKENS = len(alphabet) + 2\n",
    "seq_char_len, NUM_CHAR_TOKENS, alphabet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model s 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_data = train_data_1[\"line_number\"].to_numpy() / train_data_1[\"total_lines\"].to_numpy()\n",
    "train_pos_rounded = ((train_pos_data * 20).round(0)/20)\n",
    "\n",
    "val_pos_data = val_data_1[\"line_number\"].to_numpy() / val_data_1[\"total_lines\"].to_numpy()\n",
    "val_pos_rounded = ((val_pos_data * 20).round(0)/20)\n",
    "\n",
    "test_pos_data = test_df_100[\"line_number\"].to_numpy() / test_df_100[\"total_lines\"].to_numpy()\n",
    "test_pos_rounded = ((test_pos_data * 20).round(0)/20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word_char_pos_data = tf.data.Dataset.from_tensor_slices((train_sentences, train_chars, train_pos_rounded))\n",
    "train_word_char_pos_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot)\n",
    "train_dataset = tf.data.Dataset.zip((train_word_char_pos_data, train_word_char_pos_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_word_char_pos_data = tf.data.Dataset.from_tensor_slices((val_sentences, val_chars, val_pos_rounded))\n",
    "val_word_char_pos_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\n",
    "val_dataset = tf.data.Dataset.zip((val_word_char_pos_data, val_word_char_pos_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_word_char_pos_data = tf.data.Dataset.from_tensor_slices((test_sentences, test_chars, test_pos_rounded))\n",
    "test_word_char_pos_labels = tf.data.Dataset.from_tensor_slices(test_labels_one_hot)\n",
    "test_dataset = tf.data.Dataset.zip((test_word_char_pos_data, test_word_char_pos_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[49], line 1\u001b[0m\n",
      "\u001b[1;32m----> 1\u001b[0m hub_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mKerasLayer\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://www.kaggle.com/models/google/universal-sentence-encoder/frameworks/TensorFlow2/variations/universal-sentence-encoder/versions/2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muniversal_sentence_encoder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[0;32m      6\u001b[0m char_vectorizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mTextVectorization(\n",
      "\u001b[0;32m      7\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39mNUM_CHAR_TOKENS,\n",
      "\u001b[0;32m      8\u001b[0m     output_sequence_length\u001b[38;5;241m=\u001b[39mseq_char_len,\n",
      "\u001b[0;32m      9\u001b[0m )\n",
      "\u001b[0;32m     11\u001b[0m char_vectorizer\u001b[38;5;241m.\u001b[39madapt(train_chars)\n",
      "\n",
      "File \u001b[1;32mc:\\This_PC\\Learn\\TensorFlow\\.conda\\Lib\\site-packages\\tensorflow_hub\\keras_layer.py:165\u001b[0m, in \u001b[0;36mKerasLayer.__init__\u001b[1;34m(self, handle, trainable, arguments, _sentinel, tags, signature, signature_outputs_as_dict, output_key, output_shape, load_options, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    161\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_shape \u001b[38;5;241m=\u001b[39m data_structures\u001b[38;5;241m.\u001b[39mNoDependency(\n",
      "\u001b[0;32m    162\u001b[0m       _convert_nest_to_shapes(output_shape))\n",
      "\u001b[0;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_options \u001b[38;5;241m=\u001b[39m load_options\n",
      "\u001b[1;32m--> 165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func \u001b[38;5;241m=\u001b[39m \u001b[43mload_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_options\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    166\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_hub_module_v1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_hub_module_v1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;32m    168\u001b[0m \u001b[38;5;66;03m# Update with the defaults when using legacy TF1 Hub format.\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32mc:\\This_PC\\Learn\\TensorFlow\\.conda\\Lib\\site-packages\\tensorflow_hub\\keras_layer.py:467\u001b[0m, in \u001b[0;36mload_module\u001b[1;34m(handle, tags, load_options)\u001b[0m\n",
      "\u001b[0;32m    465\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:  \u001b[38;5;66;03m# Expected before TF2.4.\u001b[39;00m\n",
      "\u001b[0;32m    466\u001b[0m       set_load_options \u001b[38;5;241m=\u001b[39m load_options\n",
      "\u001b[1;32m--> 467\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodule_v2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mset_load_options\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32mc:\\This_PC\\Learn\\TensorFlow\\.conda\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:100\u001b[0m, in \u001b[0;36mload\u001b[1;34m(handle, tags, options)\u001b[0m\n",
      "\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[0;32m     99\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a string, got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m handle)\n",
      "\u001b[1;32m--> 100\u001b[0m module_path \u001b[38;5;241m=\u001b[39m \u001b[43mresolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    101\u001b[0m is_hub_module_v1 \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(_get_module_proto_path(module_path))\n",
      "\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tags \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_hub_module_v1:\n",
      "\n",
      "File \u001b[1;32mc:\\This_PC\\Learn\\TensorFlow\\.conda\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:55\u001b[0m, in \u001b[0;36mresolve\u001b[1;34m(handle)\u001b[0m\n",
      "\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresolve\u001b[39m(handle):\n",
      "\u001b[0;32m     32\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Resolves a module handle into a path.\u001b[39;00m\n",
      "\u001b[0;32m     33\u001b[0m \n",
      "\u001b[0;32m     34\u001b[0m \u001b[38;5;124;03m  This function works both for plain TF2 SavedModels and the legacy TF1 Hub\u001b[39;00m\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03m    A string representing the Module path.\u001b[39;00m\n",
      "\u001b[0;32m     54\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n",
      "\u001b[1;32m---> 55\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mregistry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolver\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32mc:\\This_PC\\Learn\\TensorFlow\\.conda\\Lib\\site-packages\\tensorflow_hub\\registry.py:49\u001b[0m, in \u001b[0;36mMultiImplRegister.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m impl \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impls):\n",
      "\u001b[0;32m     48\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m impl\u001b[38;5;241m.\u001b[39mis_supported(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m     50\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m     51\u001b[0m     fails\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mtype\u001b[39m(impl)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n",
      "\n",
      "File \u001b[1;32mc:\\This_PC\\Learn\\TensorFlow\\.conda\\Lib\\site-packages\\tensorflow_hub\\compressed_module_resolver.py:81\u001b[0m, in \u001b[0;36mHttpCompressedFileResolver.__call__\u001b[1;34m(self, handle)\u001b[0m\n",
      "\u001b[0;32m     77\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_urlopen(request)\n",
      "\u001b[0;32m     78\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m resolver\u001b[38;5;241m.\u001b[39mDownloadManager(handle)\u001b[38;5;241m.\u001b[39mdownload_and_uncompress(\n",
      "\u001b[0;32m     79\u001b[0m       response, tmp_dir)\n",
      "\u001b[1;32m---> 81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matomic_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_dir\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m     82\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lock_file_timeout_sec\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32mc:\\This_PC\\Learn\\TensorFlow\\.conda\\Lib\\site-packages\\tensorflow_hub\\resolver.py:421\u001b[0m, in \u001b[0;36matomic_download\u001b[1;34m(handle, download_fn, module_dir, lock_file_timeout_sec)\u001b[0m\n",
      "\u001b[0;32m    419\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading TF-Hub Module \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, handle)\n",
      "\u001b[0;32m    420\u001b[0m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mMakeDirs(tmp_dir)\n",
      "\u001b[1;32m--> 421\u001b[0m \u001b[43mdownload_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtmp_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    422\u001b[0m \u001b[38;5;66;03m# Write module descriptor to capture information about which module was\u001b[39;00m\n",
      "\u001b[0;32m    423\u001b[0m \u001b[38;5;66;03m# downloaded by whom and when. The file stored at the same level as a\u001b[39;00m\n",
      "\u001b[0;32m    424\u001b[0m \u001b[38;5;66;03m# directory in order to keep the content of the 'model_dir' exactly as it\u001b[39;00m\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    429\u001b[0m \u001b[38;5;66;03m# module caching protocol and no code in the TF-Hub library reads its\u001b[39;00m\n",
      "\u001b[0;32m    430\u001b[0m \u001b[38;5;66;03m# content.\u001b[39;00m\n",
      "\u001b[0;32m    431\u001b[0m _write_module_descriptor_file(handle, module_dir)\n",
      "\n",
      "File \u001b[1;32mc:\\This_PC\\Learn\\TensorFlow\\.conda\\Lib\\site-packages\\tensorflow_hub\\compressed_module_resolver.py:78\u001b[0m, in \u001b[0;36mHttpCompressedFileResolver.__call__.<locals>.download\u001b[1;34m(handle, tmp_dir)\u001b[0m\n",
      "\u001b[0;32m     75\u001b[0m request \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(\n",
      "\u001b[0;32m     76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_compressed_format_query(handle))\n",
      "\u001b[0;32m     77\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_urlopen(request)\n",
      "\u001b[1;32m---> 78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDownloadManager\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_uncompress\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtmp_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32mc:\\This_PC\\Learn\\TensorFlow\\.conda\\Lib\\site-packages\\tensorflow_hub\\resolver.py:192\u001b[0m, in \u001b[0;36mDownloadManager.download_and_uncompress\u001b[1;34m(self, fileobj, dst_path)\u001b[0m\n",
      "\u001b[0;32m    182\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Streams the content for the 'fileobj' and stores the result in dst_path.\u001b[39;00m\n",
      "\u001b[0;32m    183\u001b[0m \n",
      "\u001b[0;32m    184\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03m  ValueError: Unknown object encountered inside the TAR file.\u001b[39;00m\n",
      "\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m--> 192\u001b[0m   \u001b[43mfile_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_tarfile_to_destination\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m    193\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfileobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_progress\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    194\u001b[0m   total_size_str \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39mbytes_to_readable_str(\n",
      "\u001b[0;32m    195\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_total_bytes_downloaded, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;32m    196\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_print_download_progress_msg(\n",
      "\u001b[0;32m    197\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloaded \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, Total size: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_url, total_size_str),\n",
      "\u001b[0;32m    198\u001b[0m       flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\n",
      "File \u001b[1;32mc:\\This_PC\\Learn\\TensorFlow\\.conda\\Lib\\site-packages\\tensorflow_hub\\file_utils.py:52\u001b[0m, in \u001b[0;36mextract_tarfile_to_destination\u001b[1;34m(fileobj, dst_path, log_function)\u001b[0m\n",
      "\u001b[0;32m     49\u001b[0m abs_target_path \u001b[38;5;241m=\u001b[39m merge_relative_path(dst_path, tarinfo\u001b[38;5;241m.\u001b[39mname)\n",
      "\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tarinfo\u001b[38;5;241m.\u001b[39misfile():\n",
      "\u001b[1;32m---> 52\u001b[0m   \u001b[43mextract_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mabs_target_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_function\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m tarinfo\u001b[38;5;241m.\u001b[39misdir():\n",
      "\u001b[0;32m     54\u001b[0m   tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mMakeDirs(abs_target_path)\n",
      "\n",
      "File \u001b[1;32mc:\\This_PC\\Learn\\TensorFlow\\.conda\\Lib\\site-packages\\tensorflow_hub\\file_utils.py:35\u001b[0m, in \u001b[0;36mextract_file\u001b[1;34m(tgz, tarinfo, dst_path, buffer_size, log_function)\u001b[0m\n",
      "\u001b[0;32m     33\u001b[0m dst \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mGFile(dst_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;32m---> 35\u001b[0m   buf \u001b[38;5;241m=\u001b[39m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m     36\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m buf:\n",
      "\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32mc:\\This_PC\\Learn\\TensorFlow\\.conda\\Lib\\tarfile.py:700\u001b[0m, in \u001b[0;36m_FileInFile.readinto\u001b[1;34m(self, b)\u001b[0m\n",
      "\u001b[0;32m    699\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreadinto\u001b[39m(\u001b[38;5;28mself\u001b[39m, b):\n",
      "\u001b[1;32m--> 700\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    701\u001b[0m     b[:\u001b[38;5;28mlen\u001b[39m(buf)] \u001b[38;5;241m=\u001b[39m buf\n",
      "\u001b[0;32m    702\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(buf)\n",
      "\n",
      "File \u001b[1;32mc:\\This_PC\\Learn\\TensorFlow\\.conda\\Lib\\tarfile.py:689\u001b[0m, in \u001b[0;36m_FileInFile.read\u001b[1;34m(self, size)\u001b[0m\n",
      "\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data:\n",
      "\u001b[0;32m    688\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfileobj\u001b[38;5;241m.\u001b[39mseek(offset \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition \u001b[38;5;241m-\u001b[39m start))\n",
      "\u001b[1;32m--> 689\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfileobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    690\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(b) \u001b[38;5;241m!=\u001b[39m length:\n",
      "\u001b[0;32m    691\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ReadError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munexpected end of data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "File \u001b[1;32mc:\\This_PC\\Learn\\TensorFlow\\.conda\\Lib\\tarfile.py:526\u001b[0m, in \u001b[0;36m_Stream.read\u001b[1;34m(self, size)\u001b[0m\n",
      "\u001b[0;32m    524\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the next size number of bytes from the stream.\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m--> 526\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    527\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(buf)\n",
      "\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m buf\n",
      "\n",
      "File \u001b[1;32mc:\\This_PC\\Learn\\TensorFlow\\.conda\\Lib\\tarfile.py:548\u001b[0m, in \u001b[0;36m_Stream._read\u001b[1;34m(self, size)\u001b[0m\n",
      "\u001b[0;32m    546\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;32m    547\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m--> 548\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecompress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    549\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexception \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;32m    550\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvalid compressed data\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hub_embedding = hub.KerasLayer(\n",
    "    \"https://www.kaggle.com/models/google/universal-sentence-encoder/frameworks/TensorFlow2/variations/universal-sentence-encoder/versions/2\",\n",
    "    trainable=False,\n",
    "    name=\"universal_sentence_encoder\",\n",
    ")\n",
    "char_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=NUM_CHAR_TOKENS,\n",
    "    output_sequence_length=seq_char_len,\n",
    ")\n",
    "\n",
    "char_vectorizer.adapt(train_chars)\n",
    "\n",
    "char_embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=len(char_vectorizer.get_vocabulary()),\n",
    "    output_dim=25,\n",
    "    mask_zero=True,\n",
    ")\n",
    "\n",
    "token_inputs = tf.keras.layers.Input(shape=[], dtype='string', name='token_inputs')\n",
    "token_embedding = hub_embedding(token_inputs)\n",
    "token_outputs = tf.keras.layers.Dense(128, activation='relu')(token_embedding)\n",
    "token_model = tf.keras.Model(token_inputs, token_outputs)\n",
    "\n",
    "char_inputs = tf.keras.layers.Input(shape=(1,), dtype='string', name='char_inputs')\n",
    "char_vectors = char_vectorizer(char_inputs)\n",
    "char_embeddings = char_embedding(char_vectors)\n",
    "char_bi_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(24))(char_embeddings)\n",
    "char_model = tf.keras.Model(char_inputs, char_bi_lstm)\n",
    "\n",
    "\n",
    "pos_inputs = tf.keras.layers.Input(shape=(), name='pos_inputs')\n",
    "pos_outputs = tf.keras.layers.Dense(32, activation='relu')(pos_inputs)\n",
    "pos_model = tf.keras.Model(pos_inputs, pos_outputs)\n",
    "\n",
    "final_concatenate = tf.keras.layers.Concatenate(name='final_concatenate')([token_model.output, char_model.output, pos_model.output])\n",
    "\n",
    "x = tf.keras.layers.Dense(32, activation='softmax', name='preoutput_dense_layer')(final_concatenate)\n",
    "outputs = tf.keras.layers.Dense(classes_count, activation='softmax', name='output_layer')(x)\n",
    "\n",
    "model_s_22 = tf.keras.Model([token_model.input, char_model.input, pos_model.input], outputs)\n",
    "\n",
    "model_s_22.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model_s_22_history = model_s_22.fit(\n",
    "    train_dataset,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=int(len(train_dataset)),\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=int(len(val_dataset)),\n",
    "    callbacks=get_callbacks(\"model_s_22\"),\n",
    ")\n",
    "\n",
    "model_s_22.load_weights(\"skim_lit/checkpoint/model_s_22/checkpoint.ckpt\")\n",
    "\n",
    "model_s_22_val_preds = tf.argmax(model_s_22.predict(val_dataset), axis=1)\n",
    "model_s_22_val_results = calculate_results(tf.argmax(val_labels_one_hot, axis=1), model_s_22_val_preds)\n",
    "print(model_s_22_val_results)\n",
    "\n",
    "plot_loss_curves(model_s_22_history)\n",
    "\n",
    "model_s_22_preds = tf.argmax(model_s_22.predict(test_dataset), axis=1)\n",
    "model_s_22_results = calculate_results(tf.argmax(test_labels_one_hot, axis=1), model_s_22_preds)\n",
    "model_s_22_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
