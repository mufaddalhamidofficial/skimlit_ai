{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\n",
      "zsh:1: command not found: wget\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/mufaddalhamidofficial/tensorflow_course/main/helper_funcs.py\n",
    "!wget https://media.githubusercontent.com/media/mufaddalhamidofficial/skimlit_ai/main/data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from helper_funcs import create_tensorboard_callback, create_checkpoint_callback, plot_loss_curves, compare_historys, unzip_data, calculate_results\n",
    "import tensorflow_hub as hub\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.15.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "unzip_data(\"data.zip\")\n",
    "!rm -rf data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_NAME_01 = \"data/01_percent\"\n",
    "DIR_NAME_1 = \"data/1_percent\"\n",
    "DIR_NAME_10 = \"data/10_percent\"\n",
    "DIR_NAME_100 = \"data/100_percent\"\n",
    "\n",
    "test_df_100 = pd.read_csv(DIR_NAME_100 + \"/test.csv\")\n",
    "test_df_100 = test_df_100.iloc[:, 1:]\n",
    "\n",
    "test_sentences_ = test_df_100.text.to_numpy()\n",
    "test_labels_ = test_df_100.target.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(experiment_name, use_early_stopping = True, patience=10, use_tensorboard = True, use_model_checkpoint = False):\n",
    "    callbacks = [\n",
    "    ]\n",
    "    if use_tensorboard:\n",
    "        callbacks.append(\n",
    "            create_tensorboard_callback(\n",
    "                dir_name=\"skim_lit/tensorboard\",\n",
    "                experiment_name=experiment_name,\n",
    "            )\n",
    "        )\n",
    "    if use_model_checkpoint:\n",
    "        callbacks.append(\n",
    "            create_checkpoint_callback(\n",
    "                dir_name=\"skim_lit/checkpoint\",\n",
    "                experiment_name=experiment_name,\n",
    "                monitor=\"val_accuracy\",\n",
    "            )\n",
    "        )\n",
    "    if use_early_stopping:\n",
    "        \n",
    "        callbacks.append(tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_accuracy\",\n",
    "            patience=patience,\n",
    "            restore_best_weights=True,\n",
    "        ))\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model s Data Preps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get and Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_1 = pd.read_csv(DIR_NAME_1 + \"/train.csv\")\n",
    "train_sentences = train_data_1.text.to_numpy()\n",
    "train_labels = train_data_1.target.to_numpy()\n",
    "\n",
    "val_data_1 = pd.read_csv(DIR_NAME_1 + \"/val.csv\")\n",
    "val_sentences = val_data_1.text.to_numpy()\n",
    "val_labels = val_data_1.target.to_numpy()\n",
    "\n",
    "test_sentences = test_labels_.copy()\n",
    "test_labels = test_labels_.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "train_labels_one_hot = one_hot_encoder.fit_transform(train_labels.reshape(-1, 1))\n",
    "val_labels_one_hot = one_hot_encoder.transform(val_labels.reshape(-1, 1))\n",
    "test_labels_one_hot = one_hot_encoder.transform(test_labels.reshape(-1, 1))\n",
    "\n",
    "train_labels_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BACKGROUND', 'CONCLUSIONS', 'METHODS', 'OBJECTIVE', 'RESULTS'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = one_hot_encoder.categories_[0]\n",
    "classes_count = len(class_names) # type: ignore\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(243,\n",
       " 70,\n",
       " 'abcdefghijklmnopqrstuvwxyz0123456789!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "train_chars = [\" \".join(list(sentence)) for sentence in train_sentences]\n",
    "val_chars = [\" \".join(list(sentence)) for sentence in val_sentences]\n",
    "test_chars = [\" \".join(list(sentence)) for sentence in test_sentences]\n",
    "\n",
    "char_lens = [len(sentence) for sentence in train_sentences]\n",
    "\n",
    "seq_char_len = int(np.percentile(char_lens, 90))\n",
    "\n",
    "alphabet = string.ascii_lowercase + string.digits + string.punctuation\n",
    "\n",
    "NUM_CHAR_TOKENS = len(alphabet) + 2\n",
    "seq_char_len, NUM_CHAR_TOKENS, alphabet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model s 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 13:55:23.527838: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2024-02-03 13:55:23.527863: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2024-02-03 13:55:23.527868: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2024-02-03 13:55:23.527919: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-02-03 13:55:23.527945: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "train_pos_data = train_data_1[\"line_number\"].to_numpy() / train_data_1[\"total_lines\"].to_numpy()\n",
    "train_pos_rounded = ((train_pos_data * 20).round(0)/20)\n",
    "train_pos_one_hot = tf.one_hot(train_pos_rounded, depth=20)\n",
    "\n",
    "val_pos_data = val_data_1[\"line_number\"].to_numpy() / val_data_1[\"total_lines\"].to_numpy()\n",
    "val_pos_rounded = ((val_pos_data * 20).round(0)/20)\n",
    "val_pos_one_hot = tf.one_hot(val_pos_rounded, depth=20)\n",
    "\n",
    "test_pos_data = test_df_100[\"line_number\"].to_numpy() / test_df_100[\"total_lines\"].to_numpy()\n",
    "test_pos_rounded = ((test_pos_data * 20).round(0)/20)\n",
    "test_pos_one_hot = tf.one_hot(test_pos_rounded, depth=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word_char_pos_data = tf.data.Dataset.from_tensor_slices((train_sentences, train_chars, train_pos_one_hot))\n",
    "train_word_char_pos_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot)\n",
    "train_dataset = tf.data.Dataset.zip((train_word_char_pos_data, train_word_char_pos_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_word_char_pos_data = tf.data.Dataset.from_tensor_slices((val_sentences, val_chars, val_pos_one_hot))\n",
    "val_word_char_pos_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\n",
    "val_dataset = tf.data.Dataset.zip((val_word_char_pos_data, val_word_char_pos_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_word_char_pos_data = tf.data.Dataset.from_tensor_slices((test_sentences, test_chars, test_pos_one_hot))\n",
    "test_word_char_pos_labels = tf.data.Dataset.from_tensor_slices(test_labels_one_hot)\n",
    "test_dataset = tf.data.Dataset.zip((test_word_char_pos_data, test_word_char_pos_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 13:55:28.148048: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "hub_embedding = hub.KerasLayer(\n",
    "    \"https://www.kaggle.com/models/google/universal-sentence-encoder/frameworks/TensorFlow2/variations/universal-sentence-encoder/versions/2\",\n",
    "    trainable=False,\n",
    "    name=\"universal_sentence_encoder\",\n",
    ")\n",
    "char_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=NUM_CHAR_TOKENS,\n",
    "    output_sequence_length=seq_char_len,\n",
    ")\n",
    "\n",
    "char_vectorizer.adapt(train_chars)\n",
    "\n",
    "char_embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=len(char_vectorizer.get_vocabulary()),\n",
    "    output_dim=25,\n",
    "    mask_zero=True,\n",
    ")\n",
    "\n",
    "token_inputs = tf.keras.layers.Input(shape=[], dtype='string', name='token_inputs')\n",
    "token_embedding = hub_embedding(token_inputs)\n",
    "token_outputs = tf.keras.layers.Dense(128, activation='relu')(token_embedding)\n",
    "token_model = tf.keras.Model(token_inputs, token_outputs)\n",
    "\n",
    "char_inputs = tf.keras.layers.Input(shape=(1,), dtype='string', name='char_inputs')\n",
    "char_vectors = char_vectorizer(char_inputs)\n",
    "char_embeddings = char_embedding(char_vectors)\n",
    "char_bi_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(24))(char_embeddings)\n",
    "char_model = tf.keras.Model(char_inputs, char_bi_lstm)\n",
    "\n",
    "\n",
    "pos_inputs = tf.keras.layers.Input(shape=(20,), name='pos_inputs')\n",
    "pos_outputs = tf.keras.layers.Dense(32, activation='relu')(pos_inputs)\n",
    "pos_model = tf.keras.Model(pos_inputs, pos_outputs)\n",
    "\n",
    "final_concatenate = tf.keras.layers.Concatenate(name='final_concatenate')([token_model.output, char_model.output, pos_model.output])\n",
    "\n",
    "x = tf.keras.layers.Dense(32, activation='softmax', name='preoutput_dense_layer')(final_concatenate)\n",
    "outputs = tf.keras.layers.Dense(classes_count, activation='softmax', name='output_layer')(x)\n",
    "\n",
    "model_s_6 = tf.keras.Model([token_model.input, char_model.input, pos_model.input], outputs)\n",
    "\n",
    "model_s_6.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model_s_6_history = model_s_6.fit(\n",
    "    train_dataset,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=int(len(train_dataset)),\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=int(len(val_dataset)),\n",
    "    callbacks=get_callbacks(\"model_s_6\"),\n",
    ")\n",
    "\n",
    "model_s_6_val_preds = tf.argmax(model_s_6.predict(val_dataset), axis=1)\n",
    "model_s_6_val_results = calculate_results(tf.argmax(val_labels_one_hot, axis=1), model_s_6_val_preds)\n",
    "print(model_s_6_val_results)\n",
    "\n",
    "plot_loss_curves(model_s_6_history)\n",
    "\n",
    "model_s_6_preds = tf.argmax(model_s_6.predict(test_dataset), axis=1)\n",
    "model_s_6_results = calculate_results(tf.argmax(test_labels_one_hot, axis=1), model_s_6_preds)\n",
    "model_s_6_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model s 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 13:55:23.527838: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2024-02-03 13:55:23.527863: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2024-02-03 13:55:23.527868: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2024-02-03 13:55:23.527919: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-02-03 13:55:23.527945: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "train_pos_data = train_data_1[\"line_number\"].to_numpy() / train_data_1[\"total_lines\"].to_numpy()\n",
    "train_pos_rounded = ((train_pos_data * 20).round(0)/20)\n",
    "train_pos_one_hot = tf.one_hot(train_pos_rounded, depth=20)\n",
    "\n",
    "val_pos_data = val_data_1[\"line_number\"].to_numpy() / val_data_1[\"total_lines\"].to_numpy()\n",
    "val_pos_rounded = ((val_pos_data * 20).round(0)/20)\n",
    "val_pos_one_hot = tf.one_hot(val_pos_rounded, depth=20)\n",
    "\n",
    "test_pos_data = test_df_100[\"line_number\"].to_numpy() / test_df_100[\"total_lines\"].to_numpy()\n",
    "test_pos_rounded = ((test_pos_data * 20).round(0)/20)\n",
    "test_pos_one_hot = tf.one_hot(test_pos_rounded, depth=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word_pos_data = tf.data.Dataset.from_tensor_slices((train_sentences, train_pos_one_hot))\n",
    "train_word_pos_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot)\n",
    "train_dataset = tf.data.Dataset.zip((train_word_pos_data, train_word_pos_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_word_pos_data = tf.data.Dataset.from_tensor_slices((val_sentences, val_pos_one_hot))\n",
    "val_word_pos_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\n",
    "val_dataset = tf.data.Dataset.zip((val_word_pos_data, val_word_pos_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_word_pos_data = tf.data.Dataset.from_tensor_slices((test_sentences, test_pos_one_hot))\n",
    "test_word_pos_labels = tf.data.Dataset.from_tensor_slices(test_labels_one_hot)\n",
    "test_dataset = tf.data.Dataset.zip((test_word_pos_data, test_word_pos_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 13:55:28.148048: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Open a strategy scope.\n",
    "hub_embedding = hub.KerasLayer(\n",
    "    \"https://www.kaggle.com/models/google/universal-sentence-encoder/frameworks/TensorFlow2/variations/universal-sentence-encoder/versions/2\",\n",
    "    trainable=False,\n",
    "    name=\"universal_sentence_encoder\",\n",
    ")\n",
    "\n",
    "token_inputs = tf.keras.layers.Input(shape=[], dtype='string', name='token_inputs')\n",
    "token_embedding = hub_embedding(token_inputs)\n",
    "token_outputs = tf.keras.layers.Dense(128, activation='relu')(token_embedding)\n",
    "token_model = tf.keras.Model(token_inputs, token_outputs)\n",
    "\n",
    "pos_inputs = tf.keras.layers.Input(shape=(20,), name='pos_inputs')\n",
    "pos_outputs = tf.keras.layers.Dense(32, activation='relu')(pos_inputs)\n",
    "pos_model = tf.keras.Model(pos_inputs, pos_outputs)\n",
    "\n",
    "final_concatenate = tf.keras.layers.Concatenate(name='final_concatenate')([token_model.output, pos_model.output])\n",
    "\n",
    "x = tf.keras.layers.Dense(32, activation='softmax', name='preoutput_dense_layer')(final_concatenate)\n",
    "outputs = tf.keras.layers.Dense(classes_count, activation='softmax', name='output_layer')(x)\n",
    "\n",
    "model_s_7 = tf.keras.Model([token_model.input, pos_model.input], outputs)\n",
    "\n",
    "model_s_7.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model_s_7_history = model_s_7.fit(\n",
    "    train_dataset,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=int(len(train_dataset)),\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=int(len(val_dataset)),\n",
    "    callbacks=get_callbacks(\"model_s_7\"),\n",
    ")\n",
    "\n",
    "model_s_7_val_preds = tf.argmax(model_s_7.predict(val_dataset), axis=1)\n",
    "model_s_7_val_results = calculate_results(tf.argmax(val_labels_one_hot, axis=1), model_s_7_val_preds)\n",
    "print(model_s_7_val_results)\n",
    "\n",
    "plot_loss_curves(model_s_7_history)\n",
    "\n",
    "model_s_7_preds = tf.argmax(model_s_7.predict(test_dataset), axis=1)\n",
    "model_s_7_results = calculate_results(tf.argmax(test_labels_one_hot, axis=1), model_s_7_preds)\n",
    "model_s_7_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
