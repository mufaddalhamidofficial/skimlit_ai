{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/mufaddalhamidofficial/tensorflow_course/main/helper_funcs.py\n",
    "# !wget https://media.githubusercontent.com/media/mufaddalhamidofficial/skimlit_ai/main/data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from helper_funcs import create_tensorboard_callback, create_checkpoint_callback, plot_loss_curves, compare_historys, unzip_data, calculate_results\n",
    "import tensorflow_hub as hub\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unzip_data(\"data.zip\")\n",
    "# !rm -rf data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_NAME_01 = \"data/01_percent\"\n",
    "DIR_NAME_1 = \"data/1_percent\"\n",
    "DIR_NAME_10 = \"data/10_percent\"\n",
    "DIR_NAME_100 = \"data/100_percent\"\n",
    "\n",
    "test_df_100 = pd.read_csv(DIR_NAME_100 + \"/test.csv\")\n",
    "test_df_100 = test_df_100.iloc[:, 1:]\n",
    "\n",
    "test_sentences_ = test_df_100.text.to_numpy()\n",
    "test_labels_ = test_df_100.target.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(experiment_name, use_early_stopping = True, patience=10, use_tensorboard = True, use_model_checkpoint = True):\n",
    "    callbacks = [\n",
    "    ]\n",
    "    if use_tensorboard:\n",
    "        callbacks.append(\n",
    "            create_tensorboard_callback(\n",
    "                dir_name=\"skim_lit/tensorboard\",\n",
    "                experiment_name=experiment_name,\n",
    "            )\n",
    "        )\n",
    "    if use_model_checkpoint:\n",
    "        callbacks.append(\n",
    "            create_checkpoint_callback(\n",
    "                dir_name=\"skim_lit/checkpoint\",\n",
    "                experiment_name=experiment_name,\n",
    "                monitor=\"val_accuracy\",\n",
    "            )\n",
    "        )\n",
    "    if use_early_stopping:\n",
    "        \n",
    "        callbacks.append(tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_accuracy\",\n",
    "            patience=patience,\n",
    "            restore_best_weights=True,\n",
    "        ))\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model m 0: Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get and Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_10 = pd.read_csv(DIR_NAME_10 + \"/train.csv\")\n",
    "train_sentences = train_data_10.text.to_numpy()\n",
    "train_labels = train_data_10.target.to_numpy()\n",
    "\n",
    "val_data_10 = pd.read_csv(DIR_NAME_10 + \"/val.csv\")\n",
    "val_sentences = val_data_10.text.to_numpy()\n",
    "val_labels = val_data_10.target.to_numpy()\n",
    "\n",
    "test_sentences = test_labels_.copy()\n",
    "test_labels = test_labels_.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = np.array(label_encoder.fit_transform(train_labels))\n",
    "val_labels_encoded = np.array(label_encoder.transform(val_labels))\n",
    "test_labels_encoded = np.array(label_encoder.transform(test_labels))\n",
    "\n",
    "train_labels_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "model_m_0 = Pipeline(\n",
    "    [\n",
    "        (\"tfidf\", TfidfVectorizer()),\n",
    "        (\"clf\", MultinomialNB()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_m_0.fit(train_sentences, train_labels_encoded)\n",
    "\n",
    "model_m_0_val_preds = model_m_0.predict(val_sentences)\n",
    "\n",
    "model_m_0_val_results = calculate_results(val_labels_encoded, model_m_0_val_preds)\n",
    "print(model_m_0_val_results)\n",
    "\n",
    "model_m_0_preds = model_m_0.predict(test_sentences)\n",
    "model_m_0_results = calculate_results(test_labels_encoded, model_m_0_preds)\n",
    "print(model_m_0_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model m Data Preps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get and Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_10 = pd.read_csv(DIR_NAME_10 + \"/train.csv\")\n",
    "train_sentences = train_data_10.text.to_numpy()\n",
    "train_labels = train_data_10.target.to_numpy()\n",
    "\n",
    "val_data_10 = pd.read_csv(DIR_NAME_10 + \"/val.csv\")\n",
    "val_sentences = val_data_10.text.to_numpy()\n",
    "val_labels = val_data_10.target.to_numpy()\n",
    "\n",
    "test_sentences = test_sentences_.copy()\n",
    "test_labels = test_labels_.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "train_labels_one_hot = one_hot_encoder.fit_transform(train_labels.reshape(-1, 1))\n",
    "val_labels_one_hot = one_hot_encoder.transform(val_labels.reshape(-1, 1))\n",
    "test_labels_one_hot = one_hot_encoder.transform(test_labels.reshape(-1, 1))\n",
    "\n",
    "train_labels_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = list(one_hot_encoder.categories_[0])\n",
    "classes_count = len(class_names) # type: ignore\n",
    "class_names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "train_chars = [\" \".join(list(sentence)) for sentence in train_sentences]\n",
    "val_chars = [\" \".join(list(sentence)) for sentence in val_sentences]\n",
    "test_chars = [\" \".join(list(sentence)) for sentence in test_sentences]\n",
    "\n",
    "char_lens = [len(sentence.split(' ')) for sentence in train_chars]\n",
    "\n",
    "seq_char_len = int(np.percentile(char_lens, 90))\n",
    "\n",
    "alphabet = string.ascii_lowercase + string.digits + string.punctuation\n",
    "\n",
    "NUM_CHAR_TOKENS = len(alphabet) + 2\n",
    "seq_char_len, NUM_CHAR_TOKENS, alphabet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model m 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_sentences, train_labels_one_hot))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_sentences, val_labels_one_hot))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_sentences, test_labels_one_hot))\n",
    "\n",
    "train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=68000,\n",
    "    output_sequence_length=50,\n",
    ")\n",
    "\n",
    "text_vectorizer.adapt(train_sentences)\n",
    "\n",
    "embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=len(text_vectorizer.get_vocabulary()),\n",
    "    output_dim=128,\n",
    "    mask_zero=True,\n",
    ")\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = tf.keras.layers.Conv1D(64, 5, activation='relu', padding=\"same\")(x)\n",
    "x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "outputs = tf.keras.layers.Dense(classes_count, activation=\"softmax\")(x)\n",
    "\n",
    "model_m_1 = tf.keras.Model(inputs, outputs) # type: ignore\n",
    "\n",
    "model_m_1.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model_m_1_history = model_m_1.fit(\n",
    "    train_dataset,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=int(len(train_dataset)),\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=int(len(val_dataset)),\n",
    "    callbacks=get_callbacks(\"model_m_1\"),\n",
    ")\n",
    "\n",
    "model_m_1.load_weights(\"skim_lit/checkpoint/model_m_1/checkpoint.ckpt\")\n",
    "\n",
    "model_m_1_val_preds = tf.argmax(model_m_1.predict(val_sentences), axis=1)\n",
    "model_m_1_val_results = calculate_results(tf.argmax(val_labels_one_hot, axis=1), model_m_1_val_preds)\n",
    "print(model_m_1_val_results)\n",
    "\n",
    "plot_loss_curves(model_m_1_history)\n",
    "\n",
    "model_m_1_preds = tf.argmax(model_m_1.predict(test_sentences), axis=1)\n",
    "model_m_1_results = calculate_results(tf.argmax(test_labels_one_hot, axis=1), model_m_1_preds)\n",
    "model_m_1_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model m 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_line_numbers_one_hot = tf.one_hot(train_data_10[\"line_number\"].to_numpy(), depth=15)\n",
    "val_line_numbers_one_hot = tf.one_hot(val_data_10[\"line_number\"].to_numpy(), depth=15)\n",
    "test_line_numbers_one_hot = tf.one_hot(test_df_100[\"line_number\"].to_numpy(), depth=15)\n",
    "\n",
    "train_total_lines_one_hot = tf.one_hot(train_data_10[\"total_lines\"].to_numpy(), depth=20)\n",
    "val_total_lines_one_hot = tf.one_hot(val_data_10[\"total_lines\"].to_numpy(), depth=20)\n",
    "test_total_lines_one_hot = tf.one_hot(test_df_100[\"total_lines\"].to_numpy(), depth=20)\n",
    "\n",
    "train_line_numbers_one_hot.shape, train_total_lines_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word_char_pos_data = tf.data.Dataset.from_tensor_slices((train_sentences, train_chars, train_line_numbers_one_hot, train_total_lines_one_hot))\n",
    "train_word_char_pos_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot)\n",
    "train_dataset = tf.data.Dataset.zip((train_word_char_pos_data, train_word_char_pos_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_word_char_pos_data = tf.data.Dataset.from_tensor_slices((val_sentences, val_chars, val_line_numbers_one_hot, val_total_lines_one_hot))\n",
    "val_word_char_pos_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\n",
    "val_dataset = tf.data.Dataset.zip((val_word_char_pos_data, val_word_char_pos_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_word_char_pos_data = tf.data.Dataset.from_tensor_slices((test_sentences, test_chars, test_line_numbers_one_hot, test_total_lines_one_hot))\n",
    "test_word_char_pos_labels = tf.data.Dataset.from_tensor_slices(test_labels_one_hot)\n",
    "test_dataset = tf.data.Dataset.zip((test_word_char_pos_data, test_word_char_pos_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hub_embedding = hub.KerasLayer(\n",
    "    \"https://www.kaggle.com/models/google/universal-sentence-encoder/frameworks/TensorFlow2/variations/universal-sentence-encoder/versions/2\",\n",
    "    trainable=False,\n",
    "    name=\"universal_sentence_encoder\",\n",
    ")\n",
    "\n",
    "char_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=NUM_CHAR_TOKENS,\n",
    "    output_sequence_length=seq_char_len,\n",
    ")\n",
    "\n",
    "char_vectorizer.adapt(train_chars)\n",
    "\n",
    "char_embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=len(char_vectorizer.get_vocabulary()),\n",
    "    output_dim=25,\n",
    "    mask_zero=True,\n",
    ")\n",
    "\n",
    "\n",
    "token_inputs = tf.keras.layers.Input(shape=[], dtype='string', name='token_inputs')\n",
    "token_embedding = hub_embedding(token_inputs)\n",
    "token_outputs = tf.keras.layers.Dense(128, activation='relu')(token_embedding)\n",
    "token_model = tf.keras.Model(token_inputs, token_outputs)\n",
    "\n",
    "char_inputs = tf.keras.layers.Input(shape=(1,), dtype='string', name='char_inputs')\n",
    "char_vectors = char_vectorizer(char_inputs)\n",
    "char_embeddings = char_embedding(char_vectors)\n",
    "char_bi_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(24))(char_embeddings)\n",
    "char_model = tf.keras.Model(char_inputs, char_bi_lstm)\n",
    "\n",
    "line_number_inputs = tf.keras.layers.Input(shape=(15,), dtype=tf.float32, name='line_number_inputs')\n",
    "line_number_outputs = tf.keras.layers.Dense(32, activation='relu')(line_number_inputs)\n",
    "line_number_model = tf.keras.Model(line_number_inputs, line_number_outputs)\n",
    "\n",
    "total_lines_inputs = tf.keras.layers.Input(shape=(20,), name='total_lines_inputs')\n",
    "total_lines_outputs = tf.keras.layers.Dense(32, activation='relu')(total_lines_inputs)\n",
    "total_lines_model = tf.keras.Model(total_lines_inputs, total_lines_outputs)\n",
    "\n",
    "token_char_concatenate = tf.keras.layers.Concatenate(name=\"token_char_concatenate\")([token_model.output, char_model.output])\n",
    "\n",
    "drop_out = tf.keras.layers.Dense(256, activation='relu')(token_char_concatenate)\n",
    "drop_out = tf.keras.layers.Dropout(0.5)(drop_out)\n",
    "\n",
    "final_concatenate = tf.keras.layers.Concatenate(name='final_concatenate')([drop_out, line_number_model.output, total_lines_model.output])\n",
    "\n",
    "outputs = tf.keras.layers.Dense(5, activation='softmax', name='output_layer')(final_concatenate)\n",
    "\n",
    "model_m_5 = tf.keras.Model([token_model.input, char_model.input, line_number_model.input, total_lines_model.input], outputs)\n",
    "\n",
    "model_m_5.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model_m_5_history = model_m_5.fit(\n",
    "    train_dataset,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=int(len(train_dataset)),\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=int(len(val_dataset)),\n",
    "    callbacks=get_callbacks(\"model_m_5\"),\n",
    ")\n",
    "\n",
    "model_m_5.load_weights(\"skim_lit/checkpoint/model_m_5/checkpoint.ckpt\")\n",
    "\n",
    "model_m_5_val_preds = tf.argmax(model_m_5.predict(val_dataset), axis=1)\n",
    "model_m_5_val_results = calculate_results(tf.argmax(val_labels_one_hot, axis=1), model_m_5_val_preds)\n",
    "print(model_m_5_val_results)\n",
    "\n",
    "plot_loss_curves(model_m_5_history)\n",
    "\n",
    "model_m_5_preds = tf.argmax(model_m_5.predict(test_dataset), axis=1)\n",
    "model_m_5_results = calculate_results(tf.argmax(test_labels_one_hot, axis=1), model_m_5_preds)\n",
    "model_m_5_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model m 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "train_chars_puntuations = [' '.join([e for e in list(sentence) if e not in string.ascii_lowercase + ' ']) for sentence in train_sentences]\n",
    "val_chars_puntuations = [' '.join([e for e in list(sentence) if e not in string.ascii_lowercase + ' ']) for sentence in val_sentences]\n",
    "test_chars_puntuations = [' '.join([e for e in list(sentence) if e not in string.ascii_lowercase + ' ']) for sentence in test_sentences]\n",
    "\n",
    "sentence_lengths = [len(sentence.split(' ')) for sentence in train_chars_puntuations]\n",
    "# sentence_lengths\n",
    "seq_char_punctuation_len = int(np.percentile(sentence_lengths, 95) / 4) * 4\n",
    "\n",
    "char_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=None,\n",
    "    output_sequence_length=seq_char_punctuation_len,\n",
    "    standardize='lower',\n",
    ")\n",
    "\n",
    "char_vectorizer.adapt(train_chars_puntuations)\n",
    "\n",
    "train_chars_puntuations_vectorized = np.array(char_vectorizer(np.array(train_chars_puntuations)))\n",
    "val_chars_puntuations_vectorized = np.array(char_vectorizer(np.array(val_chars_puntuations)))\n",
    "test_chars_puntuations_vectorized = np.array(char_vectorizer(np.array(test_chars_puntuations)))\n",
    "\n",
    "one_hot_matrix = np.eye(len(char_vectorizer.get_vocabulary()))\n",
    "\n",
    "train_chars_puntuations_one_hot = one_hot_matrix[train_chars_puntuations_vectorized][:, :, 2:]\n",
    "val_chars_puntuations_one_hot = one_hot_matrix[val_chars_puntuations_vectorized][:, :, 2:]\n",
    "test_chars_puntuations_one_hot = one_hot_matrix[test_chars_puntuations_vectorized][:, :, 2:]\n",
    "train_chars_puntuations_one_hot.shape, val_chars_puntuations_one_hot.shape, test_chars_puntuations_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_line_numbers_one_hot = train_data_10[\"line_number\"].to_numpy()\n",
    "val_line_numbers_one_hot = val_data_10[\"line_number\"].to_numpy()\n",
    "test_line_numbers_one_hot = test_df_100[\"line_number\"].to_numpy()\n",
    "\n",
    "train_total_lines_one_hot = train_data_10[\"total_lines\"].to_numpy()\n",
    "val_total_lines_one_hot = val_data_10[\"total_lines\"].to_numpy()\n",
    "test_total_lines_one_hot = test_df_100[\"total_lines\"].to_numpy()\n",
    "\n",
    "train_line_numbers_one_hot.shape, train_total_lines_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word_char_pos_data = tf.data.Dataset.from_tensor_slices((train_sentences, train_chars_puntuations_one_hot, train_line_numbers_one_hot, train_total_lines_one_hot))\n",
    "train_word_char_pos_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot)\n",
    "train_dataset = tf.data.Dataset.zip((train_word_char_pos_data, train_word_char_pos_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_word_char_pos_data = tf.data.Dataset.from_tensor_slices((val_sentences, val_chars_puntuations_one_hot, val_line_numbers_one_hot, val_total_lines_one_hot))\n",
    "val_word_char_pos_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\n",
    "val_dataset = tf.data.Dataset.zip((val_word_char_pos_data, val_word_char_pos_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_word_char_pos_data = tf.data.Dataset.from_tensor_slices((test_sentences, test_chars_puntuations_one_hot, test_line_numbers_one_hot, test_total_lines_one_hot))\n",
    "test_word_char_pos_labels = tf.data.Dataset.from_tensor_slices(test_labels_one_hot)\n",
    "test_dataset = tf.data.Dataset.zip((test_word_char_pos_data, test_word_char_pos_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_embedding = hub.KerasLayer(\n",
    "    \"https://www.kaggle.com/models/google/universal-sentence-encoder/frameworks/TensorFlow2/variations/universal-sentence-encoder/versions/2\",\n",
    "    trainable=False,\n",
    "    name=\"universal_sentence_encoder\",\n",
    ")\n",
    "\n",
    "\n",
    "token_inputs = tf.keras.layers.Input(shape=[], dtype='string', name='token_inputs')\n",
    "token_embedding = hub_embedding(token_inputs)\n",
    "token_outputs = tf.keras.layers.Dense(128, activation='relu')(token_embedding)\n",
    "token_model = tf.keras.Model(token_inputs, token_outputs)\n",
    "\n",
    "char_inputs = tf.keras.layers.Input(shape=(20,26,), dtype=tf.float32, name='char_puntuation_inputs')\n",
    "char_bi_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(24))(char_inputs)\n",
    "char_outputs = tf.keras.layers.Dense(128, activation='relu')(char_bi_lstm)\n",
    "char_model = tf.keras.Model(char_inputs, char_outputs)\n",
    "\n",
    "line_number_inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.float32, name='line_number_inputs')\n",
    "line_number_outputs = tf.keras.layers.Dense(32, activation='relu')(line_number_inputs)\n",
    "line_number_model = tf.keras.Model(line_number_inputs, line_number_outputs)\n",
    "\n",
    "total_lines_inputs = tf.keras.layers.Input(shape=(1,), name='total_lines_inputs')\n",
    "total_lines_outputs = tf.keras.layers.Dense(32, activation='relu')(total_lines_inputs)\n",
    "total_lines_model = tf.keras.Model(total_lines_inputs, total_lines_outputs)\n",
    "\n",
    "token_char_concatenate = tf.keras.layers.Concatenate(name=\"token_char_concatenate\")([token_model.output, char_model.output])\n",
    "\n",
    "drop_out = tf.keras.layers.Dense(256, activation='relu')(token_char_concatenate)\n",
    "drop_out = tf.keras.layers.Dropout(0.5)(drop_out)\n",
    "\n",
    "final_concatenate = tf.keras.layers.Concatenate(name='final_concatenate')([drop_out, line_number_model.output, total_lines_model.output])\n",
    "\n",
    "outputs = tf.keras.layers.Dense(5, activation='softmax', name='output_layer')(final_concatenate)\n",
    "\n",
    "model_m_23 = tf.keras.Model([token_model.input, char_model.input, line_number_model.input, total_lines_model.input], outputs)\n",
    "\n",
    "model_m_23.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model_m_23_history = model_m_23.fit(\n",
    "    train_dataset,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=int(len(train_dataset)),\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=int(len(val_dataset)),\n",
    "    callbacks=get_callbacks(\"model_m_23\"),\n",
    ")\n",
    "\n",
    "model_m_23.load_weights(\"skim_lit/checkpoint/model_m_23/checkpoint.ckpt\")\n",
    "\n",
    "model_m_23_val_preds = tf.argmax(model_m_23.predict(val_dataset), axis=1)\n",
    "model_m_23_val_results = calculate_results(tf.argmax(val_labels_one_hot, axis=1), model_m_23_val_preds)\n",
    "print(model_m_23_val_results)\n",
    "\n",
    "plot_loss_curves(model_m_23_history)\n",
    "\n",
    "model_m_23_preds = tf.argmax(model_m_23.predict(test_dataset), axis=1)\n",
    "model_m_23_results = calculate_results(tf.argmax(test_labels_one_hot, axis=1), model_m_23_preds)\n",
    "model_m_23_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model m 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "train_chars_puntuations = [' '.join([e for e in list(sentence) if e not in string.ascii_lowercase + ' ']) for sentence in train_sentences]\n",
    "val_chars_puntuations = [' '.join([e for e in list(sentence) if e not in string.ascii_lowercase + ' ']) for sentence in val_sentences]\n",
    "test_chars_puntuations = [' '.join([e for e in list(sentence) if e not in string.ascii_lowercase + ' ']) for sentence in test_sentences]\n",
    "\n",
    "sentence_lengths = [len(sentence.split(' ')) for sentence in train_chars_puntuations]\n",
    "# sentence_lengths\n",
    "seq_char_punctuation_len = int(np.percentile(sentence_lengths, 95) / 4) * 4\n",
    "\n",
    "char_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=None,\n",
    "    output_sequence_length=seq_char_punctuation_len,\n",
    "    standardize='lower',\n",
    ")\n",
    "\n",
    "char_vectorizer.adapt(train_chars_puntuations)\n",
    "\n",
    "train_chars_puntuations_vectorized = np.array(char_vectorizer(np.array(train_chars_puntuations)))\n",
    "val_chars_puntuations_vectorized = np.array(char_vectorizer(np.array(val_chars_puntuations)))\n",
    "test_chars_puntuations_vectorized = np.array(char_vectorizer(np.array(test_chars_puntuations)))\n",
    "\n",
    "one_hot_matrix = np.eye(len(char_vectorizer.get_vocabulary()))\n",
    "\n",
    "train_chars_puntuations_one_hot = one_hot_matrix[train_chars_puntuations_vectorized][:, :, 2:]\n",
    "val_chars_puntuations_one_hot = one_hot_matrix[val_chars_puntuations_vectorized][:, :, 2:]\n",
    "test_chars_puntuations_one_hot = one_hot_matrix[test_chars_puntuations_vectorized][:, :, 2:]\n",
    "train_chars_puntuations_one_hot.shape, val_chars_puntuations_one_hot.shape, test_chars_puntuations_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_count(t, val):\n",
    "    elements_equal_to_value = tf.equal(t, val)\n",
    "    as_ints = tf.cast(elements_equal_to_value, tf.int32)\n",
    "    count = tf.reduce_sum(as_ints)\n",
    "    return count\n",
    "text_vectorizer = tf.keras.layers.TextVectorization(output_sequence_length=50)\n",
    "text_vectorizer.adapt(train_sentences)\n",
    "vocab = text_vectorizer.get_vocabulary()[2:]\n",
    "\n",
    "words = tf.reshape(tf.strings.split(tf.strings.regex_replace(train_sentences, r'[!\"#$%&()\\*\\+,-\\./:;<=>?@\\[\\\\\\]^_`{|}~\\']', \"\"), ' '), shape=[-1])\n",
    "counts = {word: tf_count(words, word).numpy() for word in vocab}\n",
    "counts_df = pd.DataFrame(counts.items(), columns=[\"word\", \"count\"])\n",
    "# counts_df = counts_df.sort_values(\"count\", ascending=False)\n",
    "max_tokens = counts_df[counts_df['count'] >= 5].shape[0]\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_line_numbers_one_hot = train_data_10[\"line_number\"].to_numpy()\n",
    "val_line_numbers_one_hot = val_data_10[\"line_number\"].to_numpy()\n",
    "test_line_numbers_one_hot = test_df_100[\"line_number\"].to_numpy()\n",
    "\n",
    "train_total_lines_one_hot = train_data_10[\"total_lines\"].to_numpy()\n",
    "val_total_lines_one_hot = val_data_10[\"total_lines\"].to_numpy()\n",
    "test_total_lines_one_hot = test_df_100[\"total_lines\"].to_numpy()\n",
    "\n",
    "train_line_numbers_one_hot.shape, train_total_lines_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word_char_pos_data = tf.data.Dataset.from_tensor_slices((train_sentences, train_chars_puntuations_one_hot, train_line_numbers_one_hot, train_total_lines_one_hot))\n",
    "train_word_char_pos_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot)\n",
    "train_dataset = tf.data.Dataset.zip((train_word_char_pos_data, train_word_char_pos_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_word_char_pos_data = tf.data.Dataset.from_tensor_slices((val_sentences, val_chars_puntuations_one_hot, val_line_numbers_one_hot, val_total_lines_one_hot))\n",
    "val_word_char_pos_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\n",
    "val_dataset = tf.data.Dataset.zip((val_word_char_pos_data, val_word_char_pos_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_word_char_pos_data = tf.data.Dataset.from_tensor_slices((test_sentences, test_chars_puntuations_one_hot, test_line_numbers_one_hot, test_total_lines_one_hot))\n",
    "test_word_char_pos_labels = tf.data.Dataset.from_tensor_slices(test_labels_one_hot)\n",
    "test_dataset = tf.data.Dataset.zip((test_word_char_pos_data, test_word_char_pos_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_sequence_length=50,\n",
    ")\n",
    "\n",
    "text_vectorizer.adapt(train_sentences)\n",
    "\n",
    "embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=len(text_vectorizer.get_vocabulary()),\n",
    "    output_dim=1024,\n",
    "    mask_zero=True,\n",
    ")\n",
    "\n",
    "token_inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "token_vectorization = text_vectorizer(token_inputs)\n",
    "token_embedding = embedding(token_vectorization)\n",
    "token_merge = tf.keras.layers.Flatten()(token_embedding)\n",
    "token_outputs = tf.keras.layers.Dense(128, activation='relu')(token_merge)\n",
    "token_model = tf.keras.Model(token_inputs, token_outputs)\n",
    "\n",
    "char_inputs = tf.keras.layers.Input(shape=(20,26,), dtype=tf.float32, name='char_puntuation_inputs')\n",
    "char_bi_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(24))(char_inputs)\n",
    "char_outputs = tf.keras.layers.Dense(128, activation='relu')(char_bi_lstm)\n",
    "char_model = tf.keras.Model(char_inputs, char_outputs)\n",
    "\n",
    "line_number_inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.float32, name='line_number_inputs')\n",
    "line_number_outputs = tf.keras.layers.Dense(32, activation='relu')(line_number_inputs)\n",
    "line_number_model = tf.keras.Model(line_number_inputs, line_number_outputs)\n",
    "\n",
    "total_lines_inputs = tf.keras.layers.Input(shape=(1,), name='total_lines_inputs')\n",
    "total_lines_outputs = tf.keras.layers.Dense(32, activation='relu')(total_lines_inputs)\n",
    "total_lines_model = tf.keras.Model(total_lines_inputs, total_lines_outputs)\n",
    "\n",
    "token_char_concatenate = tf.keras.layers.Concatenate(name=\"token_char_concatenate\")([token_model.output, char_model.output])\n",
    "\n",
    "drop_out = tf.keras.layers.Dense(256, activation='relu')(token_char_concatenate)\n",
    "drop_out = tf.keras.layers.Dropout(0.5)(drop_out)\n",
    "\n",
    "final_concatenate = tf.keras.layers.Concatenate(name='final_concatenate')([drop_out, line_number_model.output, total_lines_model.output])\n",
    "\n",
    "outputs = tf.keras.layers.Dense(5, activation='softmax', name='output_layer')(final_concatenate)\n",
    "\n",
    "model_m_25 = tf.keras.Model([token_model.input, char_model.input, line_number_model.input, total_lines_model.input], outputs)\n",
    "\n",
    "model_m_25.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model_m_25_history = model_m_25.fit(\n",
    "    train_dataset,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=int(len(train_dataset)),\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=int(len(val_dataset)),\n",
    "    callbacks=get_callbacks(\"model_m_25\"),\n",
    ")\n",
    "\n",
    "model_m_25.load_weights(\"skim_lit/checkpoint/model_m_25/checkpoint.ckpt\")\n",
    "\n",
    "model_m_25_val_preds = tf.argmax(model_m_25.predict(val_dataset), axis=1)\n",
    "model_m_25_val_results = calculate_results(tf.argmax(val_labels_one_hot, axis=1), model_m_25_val_preds)\n",
    "print(model_m_25_val_results)\n",
    "\n",
    "plot_loss_curves(model_m_25_history)\n",
    "\n",
    "model_m_25_preds = tf.argmax(model_m_25.predict(test_dataset), axis=1)\n",
    "model_m_25_results = calculate_results(tf.argmax(test_labels_one_hot, axis=1), model_m_25_preds)\n",
    "model_m_25_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model m 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "train_chars_puntuations = [' '.join([e for e in list(sentence) if e not in string.ascii_lowercase + ' ']) for sentence in train_sentences]\n",
    "val_chars_puntuations = [' '.join([e for e in list(sentence) if e not in string.ascii_lowercase + ' ']) for sentence in val_sentences]\n",
    "test_chars_puntuations = [' '.join([e for e in list(sentence) if e not in string.ascii_lowercase + ' ']) for sentence in test_sentences]\n",
    "\n",
    "sentence_lengths = [len(sentence.split(' ')) for sentence in train_chars_puntuations]\n",
    "# sentence_lengths\n",
    "seq_char_punctuation_len = int(np.percentile(sentence_lengths, 95) / 4) * 4\n",
    "\n",
    "char_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=None,\n",
    "    output_sequence_length=seq_char_punctuation_len,\n",
    "    standardize='lower',\n",
    ")\n",
    "\n",
    "char_vectorizer.adapt(train_chars_puntuations)\n",
    "\n",
    "train_chars_puntuations_vectorized = np.array(char_vectorizer(np.array(train_chars_puntuations)))\n",
    "val_chars_puntuations_vectorized = np.array(char_vectorizer(np.array(val_chars_puntuations)))\n",
    "test_chars_puntuations_vectorized = np.array(char_vectorizer(np.array(test_chars_puntuations)))\n",
    "\n",
    "one_hot_matrix = np.eye(len(char_vectorizer.get_vocabulary()))\n",
    "\n",
    "train_chars_puntuations_one_hot = one_hot_matrix[train_chars_puntuations_vectorized][:, :, 2:]\n",
    "val_chars_puntuations_one_hot = one_hot_matrix[val_chars_puntuations_vectorized][:, :, 2:]\n",
    "test_chars_puntuations_one_hot = one_hot_matrix[test_chars_puntuations_vectorized][:, :, 2:]\n",
    "train_chars_puntuations_one_hot.shape, val_chars_puntuations_one_hot.shape, test_chars_puntuations_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_count(t, val):\n",
    "    elements_equal_to_value = tf.equal(t, val)\n",
    "    as_ints = tf.cast(elements_equal_to_value, tf.int32)\n",
    "    count = tf.reduce_sum(as_ints)\n",
    "    return count\n",
    "text_vectorizer = tf.keras.layers.TextVectorization(output_sequence_length=50)\n",
    "text_vectorizer.adapt(train_sentences)\n",
    "vocab = text_vectorizer.get_vocabulary()[2:]\n",
    "\n",
    "words = tf.reshape(tf.strings.split(tf.strings.regex_replace(train_sentences, r'[!\"#$%&()\\*\\+,-\\./:;<=>?@\\[\\\\\\]^_`{|}~\\']', \"\"), ' '), shape=[-1])\n",
    "counts = {word: tf_count(words, word).numpy() for word in vocab}\n",
    "counts_df = pd.DataFrame(counts.items(), columns=[\"word\", \"count\"])\n",
    "# counts_df = counts_df.sort_values(\"count\", ascending=False)\n",
    "max_tokens = counts_df[counts_df['count'] >= 5].shape[0]\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_line_numbers_one_hot = train_data_10[\"line_number\"].to_numpy()\n",
    "val_line_numbers_one_hot = val_data_10[\"line_number\"].to_numpy()\n",
    "test_line_numbers_one_hot = test_df_100[\"line_number\"].to_numpy()\n",
    "\n",
    "train_total_lines_one_hot = train_data_10[\"total_lines\"].to_numpy()\n",
    "val_total_lines_one_hot = val_data_10[\"total_lines\"].to_numpy()\n",
    "test_total_lines_one_hot = test_df_100[\"total_lines\"].to_numpy()\n",
    "\n",
    "train_line_numbers_one_hot.shape, train_total_lines_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word_char_pos_data = tf.data.Dataset.from_tensor_slices((train_sentences, train_chars_puntuations_one_hot, train_line_numbers_one_hot, train_total_lines_one_hot))\n",
    "train_word_char_pos_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot)\n",
    "train_dataset = tf.data.Dataset.zip((train_word_char_pos_data, train_word_char_pos_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_word_char_pos_data = tf.data.Dataset.from_tensor_slices((val_sentences, val_chars_puntuations_one_hot, val_line_numbers_one_hot, val_total_lines_one_hot))\n",
    "val_word_char_pos_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\n",
    "val_dataset = tf.data.Dataset.zip((val_word_char_pos_data, val_word_char_pos_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_word_char_pos_data = tf.data.Dataset.from_tensor_slices((test_sentences, test_chars_puntuations_one_hot, test_line_numbers_one_hot, test_total_lines_one_hot))\n",
    "test_word_char_pos_labels = tf.data.Dataset.from_tensor_slices(test_labels_one_hot)\n",
    "test_dataset = tf.data.Dataset.zip((test_word_char_pos_data, test_word_char_pos_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_sequence_length=50,\n",
    ")\n",
    "\n",
    "text_vectorizer.adapt(train_sentences)\n",
    "\n",
    "embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=len(text_vectorizer.get_vocabulary()),\n",
    "    output_dim=1024,\n",
    "    mask_zero=True,\n",
    ")\n",
    "\n",
    "token_inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "token_vectorization = text_vectorizer(token_inputs)\n",
    "token_embedding = embedding(token_vectorization)\n",
    "token_merge = tf.keras.layers.Flatten()(token_embedding)\n",
    "token_x = tf.keras.layers.Dense(512, activation='relu')(token_merge)\n",
    "token_x = tf.keras.layers.Dense(256, activation='relu')(token_x)\n",
    "token_outputs = tf.keras.layers.Dense(128, activation='relu')(token_x)\n",
    "token_model = tf.keras.Model(token_inputs, token_outputs)\n",
    "\n",
    "char_inputs = tf.keras.layers.Input(shape=(20,26,), dtype=tf.float32, name='char_puntuation_inputs')\n",
    "char_bi_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(24))(char_inputs)\n",
    "char_outputs = tf.keras.layers.Dense(128, activation='relu')(char_bi_lstm)\n",
    "char_model = tf.keras.Model(char_inputs, char_outputs)\n",
    "\n",
    "line_number_inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.float32, name='line_number_inputs')\n",
    "line_number_outputs = tf.keras.layers.Dense(32, activation='relu')(line_number_inputs)\n",
    "line_number_model = tf.keras.Model(line_number_inputs, line_number_outputs)\n",
    "\n",
    "total_lines_inputs = tf.keras.layers.Input(shape=(1,), name='total_lines_inputs')\n",
    "total_lines_outputs = tf.keras.layers.Dense(32, activation='relu')(total_lines_inputs)\n",
    "total_lines_model = tf.keras.Model(total_lines_inputs, total_lines_outputs)\n",
    "\n",
    "token_char_concatenate = tf.keras.layers.Concatenate(name=\"token_char_concatenate\")([token_model.output, char_model.output])\n",
    "\n",
    "drop_out = tf.keras.layers.Dense(256, activation='relu')(token_char_concatenate)\n",
    "drop_out = tf.keras.layers.Dropout(0.5)(drop_out)\n",
    "\n",
    "final_concatenate = tf.keras.layers.Concatenate(name='final_concatenate')([drop_out, line_number_model.output, total_lines_model.output])\n",
    "\n",
    "outputs = tf.keras.layers.Dense(5, activation='softmax', name='output_layer')(final_concatenate)\n",
    "\n",
    "model_m_32 = tf.keras.Model([token_model.input, char_model.input, line_number_model.input, total_lines_model.input], outputs)\n",
    "\n",
    "model_m_32.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model_m_32_history = model_m_32.fit(\n",
    "    train_dataset,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=int(len(train_dataset)),\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=int(len(val_dataset)),\n",
    "    callbacks=get_callbacks(\"model_m_32\"),\n",
    ")\n",
    "\n",
    "model_m_32.load_weights(\"skim_lit/checkpoint/model_m_32/checkpoint.ckpt\")\n",
    "\n",
    "model_m_32_val_preds = tf.argmax(model_m_32.predict(val_dataset), axis=1)\n",
    "model_m_32_val_results = calculate_results(tf.argmax(val_labels_one_hot, axis=1), model_m_32_val_preds)\n",
    "print(model_m_32_val_results)\n",
    "\n",
    "plot_loss_curves(model_m_32_history)\n",
    "\n",
    "model_m_32_preds = tf.argmax(model_m_32.predict(test_dataset), axis=1)\n",
    "model_m_32_results = calculate_results(tf.argmax(test_labels_one_hot, axis=1), model_m_32_preds)\n",
    "model_m_32_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model m 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "train_chars_puntuations = [' '.join([e for e in list(sentence) if e not in string.ascii_lowercase + ' ']) for sentence in train_sentences]\n",
    "val_chars_puntuations = [' '.join([e for e in list(sentence) if e not in string.ascii_lowercase + ' ']) for sentence in val_sentences]\n",
    "test_chars_puntuations = [' '.join([e for e in list(sentence) if e not in string.ascii_lowercase + ' ']) for sentence in test_sentences]\n",
    "\n",
    "sentence_lengths = [len(sentence.split(' ')) for sentence in train_chars_puntuations]\n",
    "# sentence_lengths\n",
    "seq_char_punctuation_len = int(np.percentile(sentence_lengths, 95) / 4) * 4\n",
    "\n",
    "char_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=None,\n",
    "    output_sequence_length=seq_char_punctuation_len,\n",
    "    standardize='lower',\n",
    ")\n",
    "\n",
    "char_vectorizer.adapt(train_chars_puntuations)\n",
    "\n",
    "train_chars_puntuations_vectorized = np.array(char_vectorizer(np.array(train_chars_puntuations)))\n",
    "val_chars_puntuations_vectorized = np.array(char_vectorizer(np.array(val_chars_puntuations)))\n",
    "test_chars_puntuations_vectorized = np.array(char_vectorizer(np.array(test_chars_puntuations)))\n",
    "\n",
    "one_hot_matrix = np.eye(len(char_vectorizer.get_vocabulary()))\n",
    "\n",
    "train_chars_puntuations_one_hot = one_hot_matrix[train_chars_puntuations_vectorized][:, :, 2:]\n",
    "val_chars_puntuations_one_hot = one_hot_matrix[val_chars_puntuations_vectorized][:, :, 2:]\n",
    "test_chars_puntuations_one_hot = one_hot_matrix[test_chars_puntuations_vectorized][:, :, 2:]\n",
    "train_chars_puntuations_one_hot.shape, val_chars_puntuations_one_hot.shape, test_chars_puntuations_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_line_numbers_one_hot = train_data_10[\"line_number\"].to_numpy()\n",
    "val_line_numbers_one_hot = val_data_10[\"line_number\"].to_numpy()\n",
    "test_line_numbers_one_hot = test_df_100[\"line_number\"].to_numpy()\n",
    "\n",
    "train_total_lines_one_hot = train_data_10[\"total_lines\"].to_numpy()\n",
    "val_total_lines_one_hot = val_data_10[\"total_lines\"].to_numpy()\n",
    "test_total_lines_one_hot = test_df_100[\"total_lines\"].to_numpy()\n",
    "\n",
    "train_line_numbers_one_hot.shape, train_total_lines_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word_char_pos_data = tf.data.Dataset.from_tensor_slices((train_sentences, train_chars_puntuations_one_hot, train_line_numbers_one_hot, train_total_lines_one_hot))\n",
    "train_word_char_pos_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot)\n",
    "train_dataset = tf.data.Dataset.zip((train_word_char_pos_data, train_word_char_pos_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_word_char_pos_data = tf.data.Dataset.from_tensor_slices((val_sentences, val_chars_puntuations_one_hot, val_line_numbers_one_hot, val_total_lines_one_hot))\n",
    "val_word_char_pos_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\n",
    "val_dataset = tf.data.Dataset.zip((val_word_char_pos_data, val_word_char_pos_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_word_char_pos_data = tf.data.Dataset.from_tensor_slices((test_sentences, test_chars_puntuations_one_hot, test_line_numbers_one_hot, test_total_lines_one_hot))\n",
    "test_word_char_pos_labels = tf.data.Dataset.from_tensor_slices(test_labels_one_hot)\n",
    "test_dataset = tf.data.Dataset.zip((test_word_char_pos_data, test_word_char_pos_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_embedding = hub.KerasLayer(\n",
    "    \"https://www.kaggle.com/models/google/universal-sentence-encoder/frameworks/TensorFlow2/variations/universal-sentence-encoder/versions/2\",\n",
    "    trainable=False,\n",
    "    name=\"universal_sentence_encoder\",\n",
    ")\n",
    "\n",
    "\n",
    "token_inputs = tf.keras.layers.Input(shape=[], dtype='string', name='token_inputs')\n",
    "token_embedding = hub_embedding(token_inputs)\n",
    "token_x = tf.keras.layers.Dense(512, activation='relu')(token_embedding)\n",
    "token_x = tf.keras.layers.Dense(256, activation='relu')(token_x)\n",
    "token_outputs = tf.keras.layers.Dense(128, activation='relu')(token_x)\n",
    "token_model = tf.keras.Model(token_inputs, token_outputs)\n",
    "\n",
    "char_inputs = tf.keras.layers.Input(shape=(20,26,), dtype=tf.float32, name='char_puntuation_inputs')\n",
    "char_bi_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(24))(char_inputs)\n",
    "char_outputs = tf.keras.layers.Dense(128, activation='relu')(char_bi_lstm)\n",
    "char_model = tf.keras.Model(char_inputs, char_outputs)\n",
    "\n",
    "line_number_inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.float32, name='line_number_inputs')\n",
    "line_number_outputs = tf.keras.layers.Dense(32, activation='relu')(line_number_inputs)\n",
    "line_number_model = tf.keras.Model(line_number_inputs, line_number_outputs)\n",
    "\n",
    "total_lines_inputs = tf.keras.layers.Input(shape=(1,), name='total_lines_inputs')\n",
    "total_lines_outputs = tf.keras.layers.Dense(32, activation='relu')(total_lines_inputs)\n",
    "total_lines_model = tf.keras.Model(total_lines_inputs, total_lines_outputs)\n",
    "\n",
    "token_char_concatenate = tf.keras.layers.Concatenate(name=\"token_char_concatenate\")([token_model.output, char_model.output])\n",
    "\n",
    "drop_out = tf.keras.layers.Dense(256, activation='relu')(token_char_concatenate)\n",
    "drop_out = tf.keras.layers.Dropout(0.5)(drop_out)\n",
    "\n",
    "final_concatenate = tf.keras.layers.Concatenate(name='final_concatenate')([drop_out, line_number_model.output, total_lines_model.output])\n",
    "\n",
    "outputs = tf.keras.layers.Dense(5, activation='softmax', name='output_layer')(final_concatenate)\n",
    "\n",
    "model_m_35 = tf.keras.Model([token_model.input, char_model.input, line_number_model.input, total_lines_model.input], outputs)\n",
    "\n",
    "model_m_35.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model_m_35_history = model_m_35.fit(\n",
    "    train_dataset,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=int(len(train_dataset)),\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=int(len(val_dataset)),\n",
    "    callbacks=get_callbacks(\"model_m_35\"),\n",
    ")\n",
    "\n",
    "model_m_35.load_weights(\"skim_lit/checkpoint/model_m_35/checkpoint.ckpt\")\n",
    "\n",
    "model_m_35_val_preds = tf.argmax(model_m_35.predict(val_dataset), axis=1)\n",
    "model_m_35_val_results = calculate_results(tf.argmax(val_labels_one_hot, axis=1), model_m_35_val_preds)\n",
    "print(model_m_35_val_results)\n",
    "\n",
    "plot_loss_curves(model_m_35_history)\n",
    "\n",
    "model_m_35_preds = tf.argmax(model_m_35.predict(test_dataset), axis=1)\n",
    "model_m_35_results = calculate_results(tf.argmax(test_labels_one_hot, axis=1), model_m_35_preds)\n",
    "model_m_35_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model m 37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "train_chars_puntuations = [' '.join([e for e in list(sentence) if e not in string.ascii_lowercase + ' ']) for sentence in train_sentences]\n",
    "val_chars_puntuations = [' '.join([e for e in list(sentence) if e not in string.ascii_lowercase + ' ']) for sentence in val_sentences]\n",
    "test_chars_puntuations = [' '.join([e for e in list(sentence) if e not in string.ascii_lowercase + ' ']) for sentence in test_sentences]\n",
    "\n",
    "sentence_lengths = [len(sentence.split(' ')) for sentence in train_chars_puntuations]\n",
    "# sentence_lengths\n",
    "seq_char_punctuation_len = int(np.percentile(sentence_lengths, 95) / 4) * 4\n",
    "\n",
    "char_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=None,\n",
    "    output_sequence_length=seq_char_punctuation_len,\n",
    "    standardize='lower',\n",
    ")\n",
    "\n",
    "char_vectorizer.adapt(train_chars_puntuations)\n",
    "\n",
    "train_chars_puntuations_vectorized = np.array(char_vectorizer(np.array(train_chars_puntuations)))\n",
    "val_chars_puntuations_vectorized = np.array(char_vectorizer(np.array(val_chars_puntuations)))\n",
    "test_chars_puntuations_vectorized = np.array(char_vectorizer(np.array(test_chars_puntuations)))\n",
    "\n",
    "one_hot_matrix = np.eye(len(char_vectorizer.get_vocabulary()))\n",
    "\n",
    "train_chars_puntuations_one_hot = one_hot_matrix[train_chars_puntuations_vectorized][:, :, 2:]\n",
    "val_chars_puntuations_one_hot = one_hot_matrix[val_chars_puntuations_vectorized][:, :, 2:]\n",
    "test_chars_puntuations_one_hot = one_hot_matrix[test_chars_puntuations_vectorized][:, :, 2:]\n",
    "train_chars_puntuations_one_hot.shape, val_chars_puntuations_one_hot.shape, test_chars_puntuations_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_count(t, val):\n",
    "    elements_equal_to_value = tf.equal(t, val)\n",
    "    as_ints = tf.cast(elements_equal_to_value, tf.int32)\n",
    "    count = tf.reduce_sum(as_ints)\n",
    "    return count\n",
    "text_vectorizer = tf.keras.layers.TextVectorization(output_sequence_length=50)\n",
    "text_vectorizer.adapt(train_sentences)\n",
    "vocab = text_vectorizer.get_vocabulary()[2:]\n",
    "\n",
    "words = tf.reshape(tf.strings.split(tf.strings.regex_replace(train_sentences, r'[!\"#$%&()\\*\\+,-\\./:;<=>?@\\[\\\\\\]^_`{|}~\\']', \"\"), ' '), shape=[-1])\n",
    "counts = {word: tf_count(words, word).numpy() for word in vocab}\n",
    "counts_df = pd.DataFrame(counts.items(), columns=[\"word\", \"count\"])\n",
    "# counts_df = counts_df.sort_values(\"count\", ascending=False)\n",
    "max_tokens = counts_df[counts_df['count'] >= 5].shape[0]\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_line_numbers_one_hot = train_data_10[\"line_number\"].to_numpy()\n",
    "val_line_numbers_one_hot = val_data_10[\"line_number\"].to_numpy()\n",
    "test_line_numbers_one_hot = test_df_100[\"line_number\"].to_numpy()\n",
    "\n",
    "train_total_lines_one_hot = train_data_10[\"total_lines\"].to_numpy()\n",
    "val_total_lines_one_hot = val_data_10[\"total_lines\"].to_numpy()\n",
    "test_total_lines_one_hot = test_df_100[\"total_lines\"].to_numpy()\n",
    "\n",
    "train_line_numbers_one_hot.shape, train_total_lines_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word_char_pos_data = tf.data.Dataset.from_tensor_slices((train_sentences, train_chars_puntuations_one_hot, train_line_numbers_one_hot, train_total_lines_one_hot))\n",
    "train_word_char_pos_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot)\n",
    "train_dataset = tf.data.Dataset.zip((train_word_char_pos_data, train_word_char_pos_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_word_char_pos_data = tf.data.Dataset.from_tensor_slices((val_sentences, val_chars_puntuations_one_hot, val_line_numbers_one_hot, val_total_lines_one_hot))\n",
    "val_word_char_pos_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\n",
    "val_dataset = tf.data.Dataset.zip((val_word_char_pos_data, val_word_char_pos_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_word_char_pos_data = tf.data.Dataset.from_tensor_slices((test_sentences, test_chars_puntuations_one_hot, test_line_numbers_one_hot, test_total_lines_one_hot))\n",
    "test_word_char_pos_labels = tf.data.Dataset.from_tensor_slices(test_labels_one_hot)\n",
    "test_dataset = tf.data.Dataset.zip((test_word_char_pos_data, test_word_char_pos_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_sequence_length=50,\n",
    ")\n",
    "\n",
    "text_vectorizer.adapt(train_sentences)\n",
    "\n",
    "embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=len(text_vectorizer.get_vocabulary()),\n",
    "    output_dim=1024,\n",
    "    mask_zero=True,\n",
    ")\n",
    "\n",
    "token_inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "token_vectorization = text_vectorizer(token_inputs)\n",
    "token_embedding = embedding(token_vectorization)\n",
    "token_merge = tf.keras.layers.Flatten()(token_embedding)\n",
    "token_x = tf.keras.layers.Dense(512, activation='relu')(token_merge)\n",
    "token_x = tf.keras.layers.Dense(256, activation='relu')(token_x)\n",
    "token_outputs = tf.keras.layers.Dense(128, activation='relu')(token_x)\n",
    "token_model = tf.keras.Model(token_inputs, token_outputs)\n",
    "\n",
    "char_inputs = tf.keras.layers.Input(shape=(20,26,), dtype=tf.float32, name='char_puntuation_inputs')\n",
    "char_bi_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))(char_inputs)\n",
    "char_x = tf.keras.layers.Dense(256, activation='relu')(char_bi_lstm)\n",
    "char_outputs = tf.keras.layers.Dense(128, activation='relu')(char_x)\n",
    "char_model = tf.keras.Model(char_inputs, char_outputs)\n",
    "\n",
    "line_number_inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.float32, name='line_number_inputs')\n",
    "line_number_outputs = tf.keras.layers.Dense(32, activation='relu')(line_number_inputs)\n",
    "line_number_model = tf.keras.Model(line_number_inputs, line_number_outputs)\n",
    "\n",
    "total_lines_inputs = tf.keras.layers.Input(shape=(1,), name='total_lines_inputs')\n",
    "total_lines_outputs = tf.keras.layers.Dense(32, activation='relu')(total_lines_inputs)\n",
    "total_lines_model = tf.keras.Model(total_lines_inputs, total_lines_outputs)\n",
    "\n",
    "token_char_concatenate = tf.keras.layers.Concatenate(name=\"token_char_concatenate\")([token_model.output, char_model.output])\n",
    "\n",
    "drop_out = tf.keras.layers.Dense(256, activation='relu')(token_char_concatenate)\n",
    "drop_out = tf.keras.layers.Dropout(0.5)(drop_out)\n",
    "\n",
    "final_concatenate = tf.keras.layers.Concatenate(name='final_concatenate')([drop_out, line_number_model.output, total_lines_model.output])\n",
    "\n",
    "outputs = tf.keras.layers.Dense(5, activation='softmax', name='output_layer')(final_concatenate)\n",
    "\n",
    "model_m_37 = tf.keras.Model([token_model.input, char_model.input, line_number_model.input, total_lines_model.input], outputs)\n",
    "\n",
    "model_m_37.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model_m_37_history = model_m_37.fit(\n",
    "    train_dataset,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=int(len(train_dataset)),\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=int(len(val_dataset)),\n",
    "    callbacks=get_callbacks(\"model_m_37\"),\n",
    ")\n",
    "\n",
    "model_m_37.load_weights(\"skim_lit/checkpoint/model_m_37/checkpoint.ckpt\")\n",
    "\n",
    "model_m_37_val_preds = tf.argmax(model_m_37.predict(val_dataset), axis=1)\n",
    "model_m_37_val_results = calculate_results(tf.argmax(val_labels_one_hot, axis=1), model_m_37_val_preds)\n",
    "print(model_m_37_val_results)\n",
    "\n",
    "plot_loss_curves(model_m_37_history)\n",
    "\n",
    "model_m_37_preds = tf.argmax(model_m_37.predict(test_dataset), axis=1)\n",
    "model_m_37_results = calculate_results(tf.argmax(test_labels_one_hot, axis=1), model_m_37_preds)\n",
    "model_m_37_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model m 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "train_chars_puntuations = [' '.join([e for e in list(sentence) if e not in string.ascii_lowercase + ' ']) for sentence in train_sentences]\n",
    "val_chars_puntuations = [' '.join([e for e in list(sentence) if e not in string.ascii_lowercase + ' ']) for sentence in val_sentences]\n",
    "test_chars_puntuations = [' '.join([e for e in list(sentence) if e not in string.ascii_lowercase + ' ']) for sentence in test_sentences]\n",
    "\n",
    "sentence_lengths = [len(sentence.split(' ')) for sentence in train_chars_puntuations]\n",
    "# sentence_lengths\n",
    "seq_char_punctuation_len = int(np.percentile(sentence_lengths, 95) / 4) * 4\n",
    "\n",
    "char_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=None,\n",
    "    output_sequence_length=seq_char_punctuation_len,\n",
    "    standardize='lower',\n",
    ")\n",
    "\n",
    "char_vectorizer.adapt(train_chars_puntuations)\n",
    "\n",
    "train_chars_puntuations_vectorized = np.array(char_vectorizer(np.array(train_chars_puntuations)))\n",
    "val_chars_puntuations_vectorized = np.array(char_vectorizer(np.array(val_chars_puntuations)))\n",
    "test_chars_puntuations_vectorized = np.array(char_vectorizer(np.array(test_chars_puntuations)))\n",
    "\n",
    "one_hot_matrix = np.eye(len(char_vectorizer.get_vocabulary()))\n",
    "\n",
    "train_chars_puntuations_one_hot = one_hot_matrix[train_chars_puntuations_vectorized][:, :, 2:]\n",
    "val_chars_puntuations_one_hot = one_hot_matrix[val_chars_puntuations_vectorized][:, :, 2:]\n",
    "test_chars_puntuations_one_hot = one_hot_matrix[test_chars_puntuations_vectorized][:, :, 2:]\n",
    "train_chars_puntuations_one_hot.shape, val_chars_puntuations_one_hot.shape, test_chars_puntuations_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_line_numbers_one_hot = train_data_10[\"line_number\"].to_numpy()\n",
    "val_line_numbers_one_hot = val_data_10[\"line_number\"].to_numpy()\n",
    "test_line_numbers_one_hot = test_df_100[\"line_number\"].to_numpy()\n",
    "\n",
    "train_total_lines_one_hot = train_data_10[\"total_lines\"].to_numpy()\n",
    "val_total_lines_one_hot = val_data_10[\"total_lines\"].to_numpy()\n",
    "test_total_lines_one_hot = test_df_100[\"total_lines\"].to_numpy()\n",
    "\n",
    "train_line_numbers_one_hot.shape, train_total_lines_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word_char_pos_data = tf.data.Dataset.from_tensor_slices((train_sentences, train_chars_puntuations_one_hot, train_line_numbers_one_hot, train_total_lines_one_hot))\n",
    "train_word_char_pos_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot)\n",
    "train_dataset = tf.data.Dataset.zip((train_word_char_pos_data, train_word_char_pos_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_word_char_pos_data = tf.data.Dataset.from_tensor_slices((val_sentences, val_chars_puntuations_one_hot, val_line_numbers_one_hot, val_total_lines_one_hot))\n",
    "val_word_char_pos_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\n",
    "val_dataset = tf.data.Dataset.zip((val_word_char_pos_data, val_word_char_pos_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_word_char_pos_data = tf.data.Dataset.from_tensor_slices((test_sentences, test_chars_puntuations_one_hot, test_line_numbers_one_hot, test_total_lines_one_hot))\n",
    "test_word_char_pos_labels = tf.data.Dataset.from_tensor_slices(test_labels_one_hot)\n",
    "test_dataset = tf.data.Dataset.zip((test_word_char_pos_data, test_word_char_pos_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_embedding = hub.KerasLayer(\n",
    "    \"https://www.kaggle.com/models/google/universal-sentence-encoder/frameworks/TensorFlow2/variations/universal-sentence-encoder/versions/2\",\n",
    "    trainable=False,\n",
    "    name=\"universal_sentence_encoder\",\n",
    ")\n",
    "\n",
    "\n",
    "token_inputs = tf.keras.layers.Input(shape=[], dtype='string', name='token_inputs')\n",
    "token_embedding = hub_embedding(token_inputs)\n",
    "token_x = tf.keras.layers.Dense(512, activation='relu')(token_embedding)\n",
    "token_x = tf.keras.layers.Dense(256, activation='relu')(token_x)\n",
    "token_outputs = tf.keras.layers.Dense(128, activation='relu')(token_x)\n",
    "token_model = tf.keras.Model(token_inputs, token_outputs)\n",
    "\n",
    "char_inputs = tf.keras.layers.Input(shape=(20,26,), dtype=tf.float32, name='char_puntuation_inputs')\n",
    "char_bi_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(24))(char_inputs)\n",
    "char_outputs = tf.keras.layers.Dense(128, activation='relu')(char_bi_lstm)\n",
    "char_model = tf.keras.Model(char_inputs, char_outputs)\n",
    "\n",
    "line_number_inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.float32, name='line_number_inputs')\n",
    "line_number_outputs = tf.keras.layers.Dense(32, activation='relu')(line_number_inputs)\n",
    "line_number_model = tf.keras.Model(line_number_inputs, line_number_outputs)\n",
    "\n",
    "total_lines_inputs = tf.keras.layers.Input(shape=(1,), name='total_lines_inputs')\n",
    "total_lines_outputs = tf.keras.layers.Dense(32, activation='relu')(total_lines_inputs)\n",
    "total_lines_model = tf.keras.Model(total_lines_inputs, total_lines_outputs)\n",
    "\n",
    "token_char_concatenate = tf.keras.layers.Concatenate(name=\"token_char_concatenate\")([token_model.output, char_model.output])\n",
    "\n",
    "drop_out = tf.keras.layers.Dense(256, activation='relu')(token_char_concatenate)\n",
    "drop_out = tf.keras.layers.Dropout(0.5)(drop_out)\n",
    "\n",
    "final_concatenate = tf.keras.layers.Concatenate(name='final_concatenate')([drop_out, line_number_model.output, total_lines_model.output])\n",
    "\n",
    "x = tf.keras.layers.Dense(256, activation='relu')(final_concatenate)\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "outputs = tf.keras.layers.Dense(5, activation='softmax', name='output_layer')(x)\n",
    "\n",
    "model_m_40 = tf.keras.Model([token_model.input, char_model.input, line_number_model.input, total_lines_model.input], outputs)\n",
    "\n",
    "model_m_40.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model_m_40_history = model_m_40.fit(\n",
    "    train_dataset,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=int(len(train_dataset)),\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=int(len(val_dataset)),\n",
    "    callbacks=get_callbacks(\"model_m_40\"),\n",
    ")\n",
    "\n",
    "model_m_40.load_weights(\"skim_lit/checkpoint/model_m_40/checkpoint.ckpt\")\n",
    "\n",
    "model_m_40_val_preds = tf.argmax(model_m_40.predict(val_dataset), axis=1)\n",
    "model_m_40_val_results = calculate_results(tf.argmax(val_labels_one_hot, axis=1), model_m_40_val_preds)\n",
    "print(model_m_40_val_results)\n",
    "\n",
    "plot_loss_curves(model_m_40_history)\n",
    "\n",
    "model_m_40_preds = tf.argmax(model_m_40.predict(test_dataset), axis=1)\n",
    "model_m_40_results = calculate_results(tf.argmax(test_labels_one_hot, axis=1), model_m_40_preds)\n",
    "model_m_40_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model m 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "train_chars_puntuations = [' '.join([e for e in list(sentence) if e not in string.ascii_lowercase + ' ']) for sentence in train_sentences]\n",
    "val_chars_puntuations = [' '.join([e for e in list(sentence) if e not in string.ascii_lowercase + ' ']) for sentence in val_sentences]\n",
    "test_chars_puntuations = [' '.join([e for e in list(sentence) if e not in string.ascii_lowercase + ' ']) for sentence in test_sentences]\n",
    "\n",
    "sentence_lengths = [len(sentence.split(' ')) for sentence in train_chars_puntuations]\n",
    "# sentence_lengths\n",
    "seq_char_punctuation_len = int(np.percentile(sentence_lengths, 95) / 4) * 4\n",
    "\n",
    "char_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=None,\n",
    "    output_sequence_length=seq_char_punctuation_len,\n",
    "    standardize='lower',\n",
    ")\n",
    "\n",
    "char_vectorizer.adapt(train_chars_puntuations)\n",
    "\n",
    "train_chars_puntuations_vectorized = np.array(char_vectorizer(np.array(train_chars_puntuations)))\n",
    "val_chars_puntuations_vectorized = np.array(char_vectorizer(np.array(val_chars_puntuations)))\n",
    "test_chars_puntuations_vectorized = np.array(char_vectorizer(np.array(test_chars_puntuations)))\n",
    "\n",
    "one_hot_matrix = np.eye(len(char_vectorizer.get_vocabulary()))\n",
    "\n",
    "train_chars_puntuations_one_hot = one_hot_matrix[train_chars_puntuations_vectorized][:, :, 2:]\n",
    "val_chars_puntuations_one_hot = one_hot_matrix[val_chars_puntuations_vectorized][:, :, 2:]\n",
    "test_chars_puntuations_one_hot = one_hot_matrix[test_chars_puntuations_vectorized][:, :, 2:]\n",
    "train_chars_puntuations_one_hot.shape, val_chars_puntuations_one_hot.shape, test_chars_puntuations_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_count(t, val):\n",
    "    elements_equal_to_value = tf.equal(t, val)\n",
    "    as_ints = tf.cast(elements_equal_to_value, tf.int32)\n",
    "    count = tf.reduce_sum(as_ints)\n",
    "    return count\n",
    "text_vectorizer = tf.keras.layers.TextVectorization(output_sequence_length=50)\n",
    "text_vectorizer.adapt(train_sentences)\n",
    "vocab = text_vectorizer.get_vocabulary()[2:]\n",
    "\n",
    "words = tf.reshape(tf.strings.split(tf.strings.regex_replace(train_sentences, r'[!\"#$%&()\\*\\+,-\\./:;<=>?@\\[\\\\\\]^_`{|}~\\']', \"\"), ' '), shape=[-1])\n",
    "counts = {word: tf_count(words, word).numpy() for word in vocab}\n",
    "counts_df = pd.DataFrame(counts.items(), columns=[\"word\", \"count\"])\n",
    "# counts_df = counts_df.sort_values(\"count\", ascending=False)\n",
    "max_tokens = counts_df[counts_df['count'] >= 5].shape[0]\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_line_numbers_one_hot = train_data_10[\"line_number\"].to_numpy()\n",
    "val_line_numbers_one_hot = val_data_10[\"line_number\"].to_numpy()\n",
    "test_line_numbers_one_hot = test_df_100[\"line_number\"].to_numpy()\n",
    "\n",
    "train_total_lines_one_hot = train_data_10[\"total_lines\"].to_numpy()\n",
    "val_total_lines_one_hot = val_data_10[\"total_lines\"].to_numpy()\n",
    "test_total_lines_one_hot = test_df_100[\"total_lines\"].to_numpy()\n",
    "\n",
    "train_line_numbers_one_hot.shape, train_total_lines_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word_char_pos_data = tf.data.Dataset.from_tensor_slices((train_sentences, train_chars_puntuations_one_hot, train_line_numbers_one_hot, train_total_lines_one_hot))\n",
    "train_word_char_pos_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot)\n",
    "train_dataset = tf.data.Dataset.zip((train_word_char_pos_data, train_word_char_pos_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_word_char_pos_data = tf.data.Dataset.from_tensor_slices((val_sentences, val_chars_puntuations_one_hot, val_line_numbers_one_hot, val_total_lines_one_hot))\n",
    "val_word_char_pos_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\n",
    "val_dataset = tf.data.Dataset.zip((val_word_char_pos_data, val_word_char_pos_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_word_char_pos_data = tf.data.Dataset.from_tensor_slices((test_sentences, test_chars_puntuations_one_hot, test_line_numbers_one_hot, test_total_lines_one_hot))\n",
    "test_word_char_pos_labels = tf.data.Dataset.from_tensor_slices(test_labels_one_hot)\n",
    "test_dataset = tf.data.Dataset.zip((test_word_char_pos_data, test_word_char_pos_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_sequence_length=50,\n",
    ")\n",
    "\n",
    "text_vectorizer.adapt(train_sentences)\n",
    "\n",
    "embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=len(text_vectorizer.get_vocabulary()),\n",
    "    output_dim=1024,\n",
    "    mask_zero=True,\n",
    ")\n",
    "\n",
    "token_inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "token_vectorization = text_vectorizer(token_inputs)\n",
    "token_embedding = embedding(token_vectorization)\n",
    "token_merge = tf.keras.layers.Flatten()(token_embedding)\n",
    "token_x = tf.keras.layers.Dense(512, activation='relu')(token_merge)\n",
    "token_x = tf.keras.layers.Dense(256, activation='relu')(token_x)\n",
    "token_outputs = tf.keras.layers.Dense(128, activation='relu')(token_x)\n",
    "token_model = tf.keras.Model(token_inputs, token_outputs)\n",
    "\n",
    "char_inputs = tf.keras.layers.Input(shape=(20,26,), dtype=tf.float32, name='char_puntuation_inputs')\n",
    "char_bi_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))(char_inputs)\n",
    "char_x = tf.keras.layers.Dense(256, activation='relu')(char_bi_lstm)\n",
    "char_outputs = tf.keras.layers.Dense(128, activation='relu')(char_x)\n",
    "char_model = tf.keras.Model(char_inputs, char_outputs)\n",
    "\n",
    "line_number_inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.float32, name='line_number_inputs')\n",
    "line_number_x = tf.keras.layers.Dense(256, activation='relu')(line_number_inputs)\n",
    "line_number_x = tf.keras.layers.Dense(128, activation='relu')(line_number_x)\n",
    "line_number_outputs = tf.keras.layers.Dense(64, activation='relu')(line_number_x)\n",
    "line_number_model = tf.keras.Model(line_number_inputs, line_number_outputs)\n",
    "\n",
    "total_lines_inputs = tf.keras.layers.Input(shape=(1,), name='total_lines_inputs')\n",
    "total_lines_x = tf.keras.layers.Dense(256, activation='relu')(total_lines_inputs)\n",
    "total_lines_x = tf.keras.layers.Dense(128, activation='relu')(total_lines_x)\n",
    "total_lines_outputs = tf.keras.layers.Dense(64, activation='relu')(total_lines_x)\n",
    "total_lines_model = tf.keras.Model(total_lines_inputs, total_lines_outputs)\n",
    "\n",
    "token_char_concatenate = tf.keras.layers.Concatenate(name=\"token_char_concatenate\")([token_model.output, char_model.output])\n",
    "\n",
    "drop_out = tf.keras.layers.Dense(256, activation='relu')(token_char_concatenate)\n",
    "drop_out = tf.keras.layers.Dropout(0.5)(drop_out)\n",
    "\n",
    "final_concatenate = tf.keras.layers.Concatenate(name='final_concatenate')([drop_out, line_number_model.output, total_lines_model.output])\n",
    "\n",
    "x = tf.keras.layers.Dense(256, activation='relu')(final_concatenate)\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "outputs = tf.keras.layers.Dense(5, activation='softmax', name='output_layer')(x)\n",
    "\n",
    "model_m_43 = tf.keras.Model([token_model.input, char_model.input, line_number_model.input, total_lines_model.input], outputs)\n",
    "\n",
    "model_m_43.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model_m_43_history = model_m_43.fit(\n",
    "    train_dataset,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=int(len(train_dataset)),\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=int(len(val_dataset)),\n",
    "    callbacks=get_callbacks(\"model_m_43\"),\n",
    ")\n",
    "\n",
    "model_m_43.load_weights(\"skim_lit/checkpoint/model_m_43/checkpoint.ckpt\")\n",
    "\n",
    "model_m_43_val_preds = tf.argmax(model_m_43.predict(val_dataset), axis=1)\n",
    "model_m_43_val_results = calculate_results(tf.argmax(val_labels_one_hot, axis=1), model_m_43_val_preds)\n",
    "print(model_m_43_val_results)\n",
    "\n",
    "plot_loss_curves(model_m_43_history)\n",
    "\n",
    "model_m_43_preds = tf.argmax(model_m_43.predict(test_dataset), axis=1)\n",
    "model_m_43_results = calculate_results(tf.argmax(test_labels_one_hot, axis=1), model_m_43_preds)\n",
    "model_m_43_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model m 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "train_chars_puntuations = [' '.join([e for e in list(sentence) if e not in string.ascii_lowercase + ' ']) for sentence in train_sentences]\n",
    "val_chars_puntuations = [' '.join([e for e in list(sentence) if e not in string.ascii_lowercase + ' ']) for sentence in val_sentences]\n",
    "test_chars_puntuations = [' '.join([e for e in list(sentence) if e not in string.ascii_lowercase + ' ']) for sentence in test_sentences]\n",
    "\n",
    "sentence_lengths = [len(sentence.split(' ')) for sentence in train_chars_puntuations]\n",
    "# sentence_lengths\n",
    "seq_char_punctuation_len = int(np.percentile(sentence_lengths, 95) / 4) * 4\n",
    "\n",
    "char_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=None,\n",
    "    output_sequence_length=seq_char_punctuation_len,\n",
    "    standardize='lower',\n",
    ")\n",
    "\n",
    "char_vectorizer.adapt(train_chars_puntuations)\n",
    "\n",
    "train_chars_puntuations_vectorized = np.array(char_vectorizer(np.array(train_chars_puntuations)))\n",
    "val_chars_puntuations_vectorized = np.array(char_vectorizer(np.array(val_chars_puntuations)))\n",
    "test_chars_puntuations_vectorized = np.array(char_vectorizer(np.array(test_chars_puntuations)))\n",
    "\n",
    "one_hot_matrix = np.eye(len(char_vectorizer.get_vocabulary()))\n",
    "\n",
    "train_chars_puntuations_one_hot = one_hot_matrix[train_chars_puntuations_vectorized][:, :, 2:]\n",
    "val_chars_puntuations_one_hot = one_hot_matrix[val_chars_puntuations_vectorized][:, :, 2:]\n",
    "test_chars_puntuations_one_hot = one_hot_matrix[test_chars_puntuations_vectorized][:, :, 2:]\n",
    "train_chars_puntuations_one_hot.shape, val_chars_puntuations_one_hot.shape, test_chars_puntuations_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_count(t, val):\n",
    "    elements_equal_to_value = tf.equal(t, val)\n",
    "    as_ints = tf.cast(elements_equal_to_value, tf.int32)\n",
    "    count = tf.reduce_sum(as_ints)\n",
    "    return count\n",
    "text_vectorizer = tf.keras.layers.TextVectorization(output_sequence_length=50)\n",
    "text_vectorizer.adapt(train_sentences)\n",
    "vocab = text_vectorizer.get_vocabulary()[2:]\n",
    "\n",
    "words = tf.reshape(tf.strings.split(tf.strings.regex_replace(train_sentences, r'[!\"#$%&()\\*\\+,-\\./:;<=>?@\\[\\\\\\]^_`{|}~\\']', \"\"), ' '), shape=[-1])\n",
    "counts = {word: tf_count(words, word).numpy() for word in vocab}\n",
    "counts_df = pd.DataFrame(counts.items(), columns=[\"word\", \"count\"])\n",
    "# counts_df = counts_df.sort_values(\"count\", ascending=False)\n",
    "max_tokens = counts_df[counts_df['count'] >= 5].shape[0]\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_line_numbers_one_hot = train_data_10[\"line_number\"].to_numpy()\n",
    "val_line_numbers_one_hot = val_data_10[\"line_number\"].to_numpy()\n",
    "test_line_numbers_one_hot = test_df_100[\"line_number\"].to_numpy()\n",
    "\n",
    "train_total_lines_one_hot = train_data_10[\"total_lines\"].to_numpy()\n",
    "val_total_lines_one_hot = val_data_10[\"total_lines\"].to_numpy()\n",
    "test_total_lines_one_hot = test_df_100[\"total_lines\"].to_numpy()\n",
    "\n",
    "train_line_numbers_one_hot.shape, train_total_lines_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word_char_pos_data = tf.data.Dataset.from_tensor_slices((train_sentences, train_chars_puntuations_one_hot, train_line_numbers_one_hot, train_total_lines_one_hot))\n",
    "train_word_char_pos_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot)\n",
    "train_dataset = tf.data.Dataset.zip((train_word_char_pos_data, train_word_char_pos_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_word_char_pos_data = tf.data.Dataset.from_tensor_slices((val_sentences, val_chars_puntuations_one_hot, val_line_numbers_one_hot, val_total_lines_one_hot))\n",
    "val_word_char_pos_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\n",
    "val_dataset = tf.data.Dataset.zip((val_word_char_pos_data, val_word_char_pos_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_word_char_pos_data = tf.data.Dataset.from_tensor_slices((test_sentences, test_chars_puntuations_one_hot, test_line_numbers_one_hot, test_total_lines_one_hot))\n",
    "test_word_char_pos_labels = tf.data.Dataset.from_tensor_slices(test_labels_one_hot)\n",
    "test_dataset = tf.data.Dataset.zip((test_word_char_pos_data, test_word_char_pos_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_sequence_length=50,\n",
    ")\n",
    "\n",
    "text_vectorizer.adapt(train_sentences)\n",
    "\n",
    "embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=len(text_vectorizer.get_vocabulary()),\n",
    "    output_dim=1024,\n",
    "    mask_zero=True,\n",
    ")\n",
    "\n",
    "token_inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "token_vectorization = text_vectorizer(token_inputs)\n",
    "token_embedding = embedding(token_vectorization)\n",
    "token_merge = tf.keras.layers.Flatten()(token_embedding)\n",
    "token_x = tf.keras.layers.Dense(512, activation='relu')(token_merge)\n",
    "token_x = tf.keras.layers.Dense(256, activation='relu')(token_x)\n",
    "token_outputs = tf.keras.layers.Dense(128, activation='relu')(token_x)\n",
    "token_model = tf.keras.Model(token_inputs, token_outputs)\n",
    "\n",
    "char_inputs = tf.keras.layers.Input(shape=(20,26,), dtype=tf.float32, name='char_puntuation_inputs')\n",
    "char_bi_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))(char_inputs)\n",
    "char_x = tf.keras.layers.Dense(256, activation='relu')(char_bi_lstm)\n",
    "char_outputs = tf.keras.layers.Dense(128, activation='relu')(char_x)\n",
    "char_model = tf.keras.Model(char_inputs, char_outputs)\n",
    "\n",
    "line_number_inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.float32, name='line_number_inputs')\n",
    "line_number_outputs = tf.keras.layers.Dense(32, activation='relu')(line_number_inputs)\n",
    "line_number_model = tf.keras.Model(line_number_inputs, line_number_outputs)\n",
    "\n",
    "total_lines_inputs = tf.keras.layers.Input(shape=(1,), name='total_lines_inputs')\n",
    "total_lines_outputs = tf.keras.layers.Dense(32, activation='relu')(total_lines_inputs)\n",
    "total_lines_model = tf.keras.Model(total_lines_inputs, total_lines_outputs)\n",
    "\n",
    "token_char_concatenate = tf.keras.layers.Concatenate(name=\"token_char_concatenate\")([token_model.output, char_model.output])\n",
    "\n",
    "drop_out = tf.keras.layers.Dense(256, activation='relu')(token_char_concatenate)\n",
    "\n",
    "final_concatenate = tf.keras.layers.Concatenate(name='final_concatenate')([drop_out, line_number_model.output, total_lines_model.output])\n",
    "\n",
    "outputs = tf.keras.layers.Dense(5, activation='softmax', name='output_layer')(final_concatenate)\n",
    "\n",
    "model_m_45 = tf.keras.Model([token_model.input, char_model.input, line_number_model.input, total_lines_model.input], outputs)\n",
    "\n",
    "model_m_45.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model_m_45_history = model_m_45.fit(\n",
    "    train_dataset,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=int(len(train_dataset)),\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=int(len(val_dataset)),\n",
    "    callbacks=get_callbacks(\"model_m_45\"),\n",
    ")\n",
    "\n",
    "model_m_45.load_weights(\"skim_lit/checkpoint/model_m_45/checkpoint.ckpt\")\n",
    "\n",
    "model_m_45_val_preds = tf.argmax(model_m_45.predict(val_dataset), axis=1)\n",
    "model_m_45_val_results = calculate_results(tf.argmax(val_labels_one_hot, axis=1), model_m_45_val_preds)\n",
    "print(model_m_45_val_results)\n",
    "\n",
    "plot_loss_curves(model_m_45_history)\n",
    "\n",
    "model_m_45_preds = tf.argmax(model_m_45.predict(test_dataset), axis=1)\n",
    "model_m_45_results = calculate_results(tf.argmax(test_labels_one_hot, axis=1), model_m_45_preds)\n",
    "model_m_45_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model m 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "train_chars_puntuations = [' '.join([e for e in list(sentence) if e not in string.ascii_lowercase + ' ']) for sentence in train_sentences]\n",
    "val_chars_puntuations = [' '.join([e for e in list(sentence) if e not in string.ascii_lowercase + ' ']) for sentence in val_sentences]\n",
    "test_chars_puntuations = [' '.join([e for e in list(sentence) if e not in string.ascii_lowercase + ' ']) for sentence in test_sentences]\n",
    "\n",
    "sentence_lengths = [len(sentence.split(' ')) for sentence in train_chars_puntuations]\n",
    "# sentence_lengths\n",
    "seq_char_punctuation_len = int(np.percentile(sentence_lengths, 95) / 4) * 4\n",
    "\n",
    "char_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=None,\n",
    "    output_sequence_length=seq_char_punctuation_len,\n",
    "    standardize='lower',\n",
    ")\n",
    "\n",
    "char_vectorizer.adapt(train_chars_puntuations)\n",
    "\n",
    "train_chars_puntuations_vectorized = np.array(char_vectorizer(np.array(train_chars_puntuations)))\n",
    "val_chars_puntuations_vectorized = np.array(char_vectorizer(np.array(val_chars_puntuations)))\n",
    "test_chars_puntuations_vectorized = np.array(char_vectorizer(np.array(test_chars_puntuations)))\n",
    "\n",
    "one_hot_matrix = np.eye(len(char_vectorizer.get_vocabulary()))\n",
    "\n",
    "train_chars_puntuations_one_hot = one_hot_matrix[train_chars_puntuations_vectorized][:, :, 2:]\n",
    "val_chars_puntuations_one_hot = one_hot_matrix[val_chars_puntuations_vectorized][:, :, 2:]\n",
    "test_chars_puntuations_one_hot = one_hot_matrix[test_chars_puntuations_vectorized][:, :, 2:]\n",
    "train_chars_puntuations_one_hot.shape, val_chars_puntuations_one_hot.shape, test_chars_puntuations_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_count(t, val):\n",
    "    elements_equal_to_value = tf.equal(t, val)\n",
    "    as_ints = tf.cast(elements_equal_to_value, tf.int32)\n",
    "    count = tf.reduce_sum(as_ints)\n",
    "    return count\n",
    "text_vectorizer = tf.keras.layers.TextVectorization(output_sequence_length=50)\n",
    "text_vectorizer.adapt(train_sentences)\n",
    "vocab = text_vectorizer.get_vocabulary()[2:]\n",
    "\n",
    "words = tf.reshape(tf.strings.split(tf.strings.regex_replace(train_sentences, r'[!\"#$%&()\\*\\+,-\\./:;<=>?@\\[\\\\\\]^_`{|}~\\']', \"\"), ' '), shape=[-1])\n",
    "counts = {word: tf_count(words, word).numpy() for word in vocab}\n",
    "counts_df = pd.DataFrame(counts.items(), columns=[\"word\", \"count\"])\n",
    "# counts_df = counts_df.sort_values(\"count\", ascending=False)\n",
    "max_tokens = counts_df[counts_df['count'] >= 5].shape[0]\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_line_numbers_one_hot = train_data_10[\"line_number\"].to_numpy()\n",
    "val_line_numbers_one_hot = val_data_10[\"line_number\"].to_numpy()\n",
    "test_line_numbers_one_hot = test_df_100[\"line_number\"].to_numpy()\n",
    "\n",
    "train_total_lines_one_hot = train_data_10[\"total_lines\"].to_numpy()\n",
    "val_total_lines_one_hot = val_data_10[\"total_lines\"].to_numpy()\n",
    "test_total_lines_one_hot = test_df_100[\"total_lines\"].to_numpy()\n",
    "\n",
    "train_line_numbers_one_hot.shape, train_total_lines_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word_char_pos_data = tf.data.Dataset.from_tensor_slices((train_sentences, train_chars_puntuations_one_hot, train_line_numbers_one_hot, train_total_lines_one_hot))\n",
    "train_word_char_pos_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot)\n",
    "train_dataset = tf.data.Dataset.zip((train_word_char_pos_data, train_word_char_pos_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_word_char_pos_data = tf.data.Dataset.from_tensor_slices((val_sentences, val_chars_puntuations_one_hot, val_line_numbers_one_hot, val_total_lines_one_hot))\n",
    "val_word_char_pos_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\n",
    "val_dataset = tf.data.Dataset.zip((val_word_char_pos_data, val_word_char_pos_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_word_char_pos_data = tf.data.Dataset.from_tensor_slices((test_sentences, test_chars_puntuations_one_hot, test_line_numbers_one_hot, test_total_lines_one_hot))\n",
    "test_word_char_pos_labels = tf.data.Dataset.from_tensor_slices(test_labels_one_hot)\n",
    "test_dataset = tf.data.Dataset.zip((test_word_char_pos_data, test_word_char_pos_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_sequence_length=50,\n",
    ")\n",
    "\n",
    "text_vectorizer.adapt(train_sentences)\n",
    "\n",
    "embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=len(text_vectorizer.get_vocabulary()),\n",
    "    output_dim=1024,\n",
    "    mask_zero=True,\n",
    ")\n",
    "\n",
    "token_inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "token_vectorization = text_vectorizer(token_inputs)\n",
    "token_embedding = embedding(token_vectorization)\n",
    "token_merge = tf.keras.layers.Flatten()(token_embedding)\n",
    "token_x = tf.keras.layers.Dense(512, activation='relu')(token_merge)\n",
    "token_x = tf.keras.layers.Dense(256, activation='relu')(token_x)\n",
    "token_outputs = tf.keras.layers.Dense(128, activation='relu')(token_x)\n",
    "token_model = tf.keras.Model(token_inputs, token_outputs)\n",
    "\n",
    "char_inputs = tf.keras.layers.Input(shape=(20,26,), dtype=tf.float32, name='char_puntuation_inputs')\n",
    "char_bi_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))(char_inputs)\n",
    "char_x = tf.keras.layers.Dense(256, activation='relu')(char_bi_lstm)\n",
    "char_outputs = tf.keras.layers.Dense(128, activation='relu')(char_x)\n",
    "char_model = tf.keras.Model(char_inputs, char_outputs)\n",
    "\n",
    "line_number_inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.float32, name='line_number_inputs')\n",
    "line_number_outputs = tf.keras.layers.Dense(32, activation='relu')(line_number_inputs)\n",
    "line_number_model = tf.keras.Model(line_number_inputs, line_number_outputs)\n",
    "\n",
    "total_lines_inputs = tf.keras.layers.Input(shape=(1,), name='total_lines_inputs')\n",
    "total_lines_outputs = tf.keras.layers.Dense(32, activation='relu')(total_lines_inputs)\n",
    "total_lines_model = tf.keras.Model(total_lines_inputs, total_lines_outputs)\n",
    "\n",
    "token_char_concatenate = tf.keras.layers.Concatenate(name=\"token_char_concatenate\")([token_model.output, char_model.output])\n",
    "\n",
    "drop_out = tf.keras.layers.Dense(256, activation='relu')(token_char_concatenate)\n",
    "\n",
    "final_concatenate = tf.keras.layers.Concatenate(name='final_concatenate')([drop_out, line_number_model.output, total_lines_model.output])\n",
    "\n",
    "outputs = tf.keras.layers.Dense(5, activation='softmax', name='output_layer')(final_concatenate)\n",
    "\n",
    "model_m_51 = tf.keras.Model([token_model.input, char_model.input, line_number_model.input, total_lines_model.input], outputs)\n",
    "\n",
    "model_m_51.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model_m_51_history = model_m_51.fit(\n",
    "    train_dataset,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=int(len(train_dataset)),\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=int(len(val_dataset)),\n",
    "    callbacks=get_callbacks(\"model_m_51\"),\n",
    ")\n",
    "\n",
    "model_m_51.load_weights(\"skim_lit/checkpoint/model_m_51/checkpoint.ckpt\")\n",
    "\n",
    "model_m_51_val_preds = tf.argmax(model_m_51.predict(val_dataset), axis=1)\n",
    "model_m_51_val_results = calculate_results(tf.argmax(val_labels_one_hot, axis=1), model_m_51_val_preds)\n",
    "print(model_m_51_val_results)\n",
    "\n",
    "plot_loss_curves(model_m_51_history)\n",
    "\n",
    "model_m_51_preds = tf.argmax(model_m_51.predict(test_dataset), axis=1)\n",
    "model_m_51_results = calculate_results(tf.argmax(test_labels_one_hot, axis=1), model_m_51_preds)\n",
    "model_m_51_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model m 54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "train_chars_puntuations = [' '.join([e for e in list(sentence) if e not in string.ascii_lowercase + ' ']) for sentence in train_sentences]\n",
    "val_chars_puntuations = [' '.join([e for e in list(sentence) if e not in string.ascii_lowercase + ' ']) for sentence in val_sentences]\n",
    "test_chars_puntuations = [' '.join([e for e in list(sentence) if e not in string.ascii_lowercase + ' ']) for sentence in test_sentences]\n",
    "\n",
    "sentence_lengths = [len(sentence.split(' ')) for sentence in train_chars_puntuations]\n",
    "# sentence_lengths\n",
    "seq_char_punctuation_len = int(np.percentile(sentence_lengths, 95) / 4) * 4\n",
    "\n",
    "char_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=None,\n",
    "    output_sequence_length=seq_char_punctuation_len,\n",
    "    standardize='lower',\n",
    ")\n",
    "\n",
    "char_vectorizer.adapt(train_chars_puntuations)\n",
    "\n",
    "train_chars_puntuations_vectorized = np.array(char_vectorizer(np.array(train_chars_puntuations)))\n",
    "val_chars_puntuations_vectorized = np.array(char_vectorizer(np.array(val_chars_puntuations)))\n",
    "test_chars_puntuations_vectorized = np.array(char_vectorizer(np.array(test_chars_puntuations)))\n",
    "\n",
    "one_hot_matrix = np.eye(len(char_vectorizer.get_vocabulary()))\n",
    "\n",
    "train_chars_puntuations_one_hot = one_hot_matrix[train_chars_puntuations_vectorized][:, :, 2:]\n",
    "val_chars_puntuations_one_hot = one_hot_matrix[val_chars_puntuations_vectorized][:, :, 2:]\n",
    "test_chars_puntuations_one_hot = one_hot_matrix[test_chars_puntuations_vectorized][:, :, 2:]\n",
    "train_chars_puntuations_one_hot.shape, val_chars_puntuations_one_hot.shape, test_chars_puntuations_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_count(t, val):\n",
    "    elements_equal_to_value = tf.equal(t, val)\n",
    "    as_ints = tf.cast(elements_equal_to_value, tf.int32)\n",
    "    count = tf.reduce_sum(as_ints)\n",
    "    return count\n",
    "text_vectorizer = tf.keras.layers.TextVectorization(output_sequence_length=50)\n",
    "text_vectorizer.adapt(train_sentences)\n",
    "vocab = text_vectorizer.get_vocabulary()[2:]\n",
    "\n",
    "words = tf.reshape(tf.strings.split(tf.strings.regex_replace(train_sentences, r'[!\"#$%&()\\*\\+,-\\./:;<=>?@\\[\\\\\\]^_`{|}~\\']', \"\"), ' '), shape=[-1])\n",
    "counts = {word: tf_count(words, word).numpy() for word in vocab}\n",
    "counts_df = pd.DataFrame(counts.items(), columns=[\"word\", \"count\"])\n",
    "# counts_df = counts_df.sort_values(\"count\", ascending=False)\n",
    "max_tokens = counts_df[counts_df['count'] >= 5].shape[0]\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_line_numbers_one_hot = train_data_10[\"line_number\"].to_numpy()\n",
    "val_line_numbers_one_hot = val_data_10[\"line_number\"].to_numpy()\n",
    "test_line_numbers_one_hot = test_df_100[\"line_number\"].to_numpy()\n",
    "\n",
    "train_total_lines_one_hot = train_data_10[\"total_lines\"].to_numpy()\n",
    "val_total_lines_one_hot = val_data_10[\"total_lines\"].to_numpy()\n",
    "test_total_lines_one_hot = test_df_100[\"total_lines\"].to_numpy()\n",
    "\n",
    "train_line_numbers_one_hot.shape, train_total_lines_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word_char_pos_data = tf.data.Dataset.from_tensor_slices((train_sentences, train_chars_puntuations_one_hot, train_line_numbers_one_hot, train_total_lines_one_hot))\n",
    "train_word_char_pos_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot)\n",
    "train_dataset = tf.data.Dataset.zip((train_word_char_pos_data, train_word_char_pos_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_word_char_pos_data = tf.data.Dataset.from_tensor_slices((val_sentences, val_chars_puntuations_one_hot, val_line_numbers_one_hot, val_total_lines_one_hot))\n",
    "val_word_char_pos_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\n",
    "val_dataset = tf.data.Dataset.zip((val_word_char_pos_data, val_word_char_pos_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_word_char_pos_data = tf.data.Dataset.from_tensor_slices((test_sentences, test_chars_puntuations_one_hot, test_line_numbers_one_hot, test_total_lines_one_hot))\n",
    "test_word_char_pos_labels = tf.data.Dataset.from_tensor_slices(test_labels_one_hot)\n",
    "test_dataset = tf.data.Dataset.zip((test_word_char_pos_data, test_word_char_pos_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_sequence_length=50,\n",
    ")\n",
    "\n",
    "text_vectorizer.adapt(train_sentences)\n",
    "\n",
    "embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=len(text_vectorizer.get_vocabulary()),\n",
    "    output_dim=1024,\n",
    "    mask_zero=True,\n",
    ")\n",
    "\n",
    "token_inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "token_vectorization = text_vectorizer(token_inputs)\n",
    "token_embedding = embedding(token_vectorization)\n",
    "token_merge = tf.keras.layers.Flatten()(token_embedding)\n",
    "token_x = tf.keras.layers.Dense(4096, activation='relu')(token_merge)\n",
    "token_x = tf.keras.layers.Dense(1024, activation='relu')(token_merge)\n",
    "token_x = tf.keras.layers.Dense(512, activation='relu')(token_merge)\n",
    "token_outputs = tf.keras.layers.Dense(128, activation='relu')(token_x)\n",
    "token_model = tf.keras.Model(token_inputs, token_outputs)\n",
    "\n",
    "char_inputs = tf.keras.layers.Input(shape=(20,26,), dtype=tf.float32, name='char_puntuation_inputs')\n",
    "char_bi_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))(char_inputs)\n",
    "char_x = tf.keras.layers.Dense(256, activation='relu')(char_bi_lstm)\n",
    "char_outputs = tf.keras.layers.Dense(128, activation='relu')(char_x)\n",
    "char_model = tf.keras.Model(char_inputs, char_outputs)\n",
    "\n",
    "line_number_inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.float32, name='line_number_inputs')\n",
    "line_number_outputs = tf.keras.layers.Dense(32, activation='relu')(line_number_inputs)\n",
    "line_number_model = tf.keras.Model(line_number_inputs, line_number_outputs)\n",
    "\n",
    "total_lines_inputs = tf.keras.layers.Input(shape=(1,), name='total_lines_inputs')\n",
    "total_lines_outputs = tf.keras.layers.Dense(32, activation='relu')(total_lines_inputs)\n",
    "total_lines_model = tf.keras.Model(total_lines_inputs, total_lines_outputs)\n",
    "\n",
    "token_char_concatenate = tf.keras.layers.Concatenate(name=\"token_char_concatenate\")([token_model.output, char_model.output])\n",
    "\n",
    "drop_out = tf.keras.layers.Dense(256, activation='relu')(token_char_concatenate)\n",
    "drop_out = tf.keras.layers.Dropout(0.5)(drop_out)\n",
    "\n",
    "final_concatenate = tf.keras.layers.Concatenate(name='final_concatenate')([drop_out, line_number_model.output, total_lines_model.output])\n",
    "\n",
    "outputs = tf.keras.layers.Dense(5, activation='softmax', name='output_layer')(final_concatenate)\n",
    "\n",
    "model_m_54 = tf.keras.Model([token_model.input, char_model.input, line_number_model.input, total_lines_model.input], outputs)\n",
    "\n",
    "model_m_54.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model_m_54_history = model_m_54.fit(\n",
    "    train_dataset,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=int(len(train_dataset)),\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=int(len(val_dataset)),\n",
    "    callbacks=get_callbacks(\"model_m_54\"),\n",
    ")\n",
    "\n",
    "model_m_54.load_weights(\"skim_lit/checkpoint/model_m_54/checkpoint.ckpt\")\n",
    "\n",
    "model_m_54_val_preds = tf.argmax(model_m_54.predict(val_dataset), axis=1)\n",
    "model_m_54_val_results = calculate_results(tf.argmax(val_labels_one_hot, axis=1), model_m_54_val_preds)\n",
    "print(model_m_54_val_results)\n",
    "\n",
    "plot_loss_curves(model_m_54_history)\n",
    "\n",
    "model_m_54_preds = tf.argmax(model_m_54.predict(test_dataset), axis=1)\n",
    "model_m_54_results = calculate_results(tf.argmax(test_labels_one_hot, axis=1), model_m_54_preds)\n",
    "model_m_54_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model m 55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "train_chars_puntuations = [' '.join([e for e in list(sentence) if e not in string.ascii_lowercase + ' ']) for sentence in train_sentences]\n",
    "val_chars_puntuations = [' '.join([e for e in list(sentence) if e not in string.ascii_lowercase + ' ']) for sentence in val_sentences]\n",
    "test_chars_puntuations = [' '.join([e for e in list(sentence) if e not in string.ascii_lowercase + ' ']) for sentence in test_sentences]\n",
    "\n",
    "sentence_lengths = [len(sentence.split(' ')) for sentence in train_chars_puntuations]\n",
    "# sentence_lengths\n",
    "seq_char_punctuation_len = int(np.percentile(sentence_lengths, 95) / 4) * 4\n",
    "\n",
    "char_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=None,\n",
    "    output_sequence_length=seq_char_punctuation_len,\n",
    "    standardize='lower',\n",
    ")\n",
    "\n",
    "char_vectorizer.adapt(train_chars_puntuations)\n",
    "\n",
    "train_chars_puntuations_vectorized = np.array(char_vectorizer(np.array(train_chars_puntuations)))\n",
    "val_chars_puntuations_vectorized = np.array(char_vectorizer(np.array(val_chars_puntuations)))\n",
    "test_chars_puntuations_vectorized = np.array(char_vectorizer(np.array(test_chars_puntuations)))\n",
    "\n",
    "one_hot_matrix = np.eye(len(char_vectorizer.get_vocabulary()))\n",
    "\n",
    "train_chars_puntuations_one_hot = one_hot_matrix[train_chars_puntuations_vectorized][:, :, 2:]\n",
    "val_chars_puntuations_one_hot = one_hot_matrix[val_chars_puntuations_vectorized][:, :, 2:]\n",
    "test_chars_puntuations_one_hot = one_hot_matrix[test_chars_puntuations_vectorized][:, :, 2:]\n",
    "train_chars_puntuations_one_hot.shape, val_chars_puntuations_one_hot.shape, test_chars_puntuations_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_count(t, val):\n",
    "    elements_equal_to_value = tf.equal(t, val)\n",
    "    as_ints = tf.cast(elements_equal_to_value, tf.int32)\n",
    "    count = tf.reduce_sum(as_ints)\n",
    "    return count\n",
    "text_vectorizer = tf.keras.layers.TextVectorization(output_sequence_length=50)\n",
    "text_vectorizer.adapt(train_sentences)\n",
    "vocab = text_vectorizer.get_vocabulary()[2:]\n",
    "\n",
    "words = tf.reshape(tf.strings.split(tf.strings.regex_replace(train_sentences, r'[!\"#$%&()\\*\\+,-\\./:;<=>?@\\[\\\\\\]^_`{|}~\\']', \"\"), ' '), shape=[-1])\n",
    "counts = {word: tf_count(words, word).numpy() for word in vocab}\n",
    "counts_df = pd.DataFrame(counts.items(), columns=[\"word\", \"count\"])\n",
    "# counts_df = counts_df.sort_values(\"count\", ascending=False)\n",
    "max_tokens = counts_df[counts_df['count'] >= 5].shape[0]\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_line_numbers_one_hot = train_data_10[\"line_number\"].to_numpy()\n",
    "val_line_numbers_one_hot = val_data_10[\"line_number\"].to_numpy()\n",
    "test_line_numbers_one_hot = test_df_100[\"line_number\"].to_numpy()\n",
    "\n",
    "train_total_lines_one_hot = train_data_10[\"total_lines\"].to_numpy()\n",
    "val_total_lines_one_hot = val_data_10[\"total_lines\"].to_numpy()\n",
    "test_total_lines_one_hot = test_df_100[\"total_lines\"].to_numpy()\n",
    "\n",
    "train_line_numbers_one_hot.shape, train_total_lines_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word_char_pos_data = tf.data.Dataset.from_tensor_slices((train_sentences, train_chars_puntuations_one_hot, train_line_numbers_one_hot, train_total_lines_one_hot))\n",
    "train_word_char_pos_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot)\n",
    "train_dataset = tf.data.Dataset.zip((train_word_char_pos_data, train_word_char_pos_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_word_char_pos_data = tf.data.Dataset.from_tensor_slices((val_sentences, val_chars_puntuations_one_hot, val_line_numbers_one_hot, val_total_lines_one_hot))\n",
    "val_word_char_pos_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\n",
    "val_dataset = tf.data.Dataset.zip((val_word_char_pos_data, val_word_char_pos_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_word_char_pos_data = tf.data.Dataset.from_tensor_slices((test_sentences, test_chars_puntuations_one_hot, test_line_numbers_one_hot, test_total_lines_one_hot))\n",
    "test_word_char_pos_labels = tf.data.Dataset.from_tensor_slices(test_labels_one_hot)\n",
    "test_dataset = tf.data.Dataset.zip((test_word_char_pos_data, test_word_char_pos_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_sequence_length=50,\n",
    ")\n",
    "\n",
    "text_vectorizer.adapt(train_sentences)\n",
    "\n",
    "embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=len(text_vectorizer.get_vocabulary()),\n",
    "    output_dim=2048,\n",
    "    mask_zero=True,\n",
    ")\n",
    "\n",
    "token_inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "token_vectorization = text_vectorizer(token_inputs)\n",
    "token_embedding = embedding(token_vectorization)\n",
    "token_merge = tf.keras.layers.Flatten()(token_embedding)\n",
    "token_x = tf.keras.layers.Dense(512, activation='relu')(token_merge)\n",
    "token_x = tf.keras.layers.Dense(256, activation='relu')(token_x)\n",
    "token_outputs = tf.keras.layers.Dense(128, activation='relu')(token_x)\n",
    "token_model = tf.keras.Model(token_inputs, token_outputs)\n",
    "\n",
    "char_inputs = tf.keras.layers.Input(shape=(20,26,), dtype=tf.float32, name='char_puntuation_inputs')\n",
    "char_bi_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))(char_inputs)\n",
    "char_x = tf.keras.layers.Dense(256, activation='relu')(char_bi_lstm)\n",
    "char_outputs = tf.keras.layers.Dense(128, activation='relu')(char_x)\n",
    "char_model = tf.keras.Model(char_inputs, char_outputs)\n",
    "\n",
    "line_number_inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.float32, name='line_number_inputs')\n",
    "line_number_outputs = tf.keras.layers.Dense(32, activation='relu')(line_number_inputs)\n",
    "line_number_model = tf.keras.Model(line_number_inputs, line_number_outputs)\n",
    "\n",
    "total_lines_inputs = tf.keras.layers.Input(shape=(1,), name='total_lines_inputs')\n",
    "total_lines_outputs = tf.keras.layers.Dense(32, activation='relu')(total_lines_inputs)\n",
    "total_lines_model = tf.keras.Model(total_lines_inputs, total_lines_outputs)\n",
    "\n",
    "token_char_concatenate = tf.keras.layers.Concatenate(name=\"token_char_concatenate\")([token_model.output, char_model.output])\n",
    "\n",
    "drop_out = tf.keras.layers.Dense(256, activation='relu')(token_char_concatenate)\n",
    "drop_out = tf.keras.layers.Dropout(0.5)(drop_out)\n",
    "\n",
    "final_concatenate = tf.keras.layers.Concatenate(name='final_concatenate')([drop_out, line_number_model.output, total_lines_model.output])\n",
    "\n",
    "outputs = tf.keras.layers.Dense(5, activation='softmax', name='output_layer')(final_concatenate)\n",
    "\n",
    "model_m_55 = tf.keras.Model([token_model.input, char_model.input, line_number_model.input, total_lines_model.input], outputs)\n",
    "\n",
    "model_m_55.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model_m_55_history = model_m_55.fit(\n",
    "    train_dataset,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=int(len(train_dataset)),\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=int(len(val_dataset)),\n",
    "    callbacks=get_callbacks(\"model_m_55\"),\n",
    ")\n",
    "\n",
    "model_m_55.load_weights(\"skim_lit/checkpoint/model_m_55/checkpoint.ckpt\")\n",
    "\n",
    "model_m_55_val_preds = tf.argmax(model_m_55.predict(val_dataset), axis=1)\n",
    "model_m_55_val_results = calculate_results(tf.argmax(val_labels_one_hot, axis=1), model_m_55_val_preds)\n",
    "print(model_m_55_val_results)\n",
    "\n",
    "plot_loss_curves(model_m_55_history)\n",
    "\n",
    "model_m_55_preds = tf.argmax(model_m_55.predict(test_dataset), axis=1)\n",
    "model_m_55_results = calculate_results(tf.argmax(test_labels_one_hot, axis=1), model_m_55_preds)\n",
    "model_m_55_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model m 56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "train_chars_puntuations = [' '.join([e for e in list(sentence) if e not in string.ascii_lowercase + ' ']) for sentence in train_sentences]\n",
    "val_chars_puntuations = [' '.join([e for e in list(sentence) if e not in string.ascii_lowercase + ' ']) for sentence in val_sentences]\n",
    "test_chars_puntuations = [' '.join([e for e in list(sentence) if e not in string.ascii_lowercase + ' ']) for sentence in test_sentences]\n",
    "\n",
    "sentence_lengths = [len(sentence.split(' ')) for sentence in train_chars_puntuations]\n",
    "# sentence_lengths\n",
    "seq_char_punctuation_len = int(np.percentile(sentence_lengths, 95) / 4) * 4\n",
    "\n",
    "char_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=None,\n",
    "    output_sequence_length=seq_char_punctuation_len,\n",
    "    standardize='lower',\n",
    ")\n",
    "\n",
    "char_vectorizer.adapt(train_chars_puntuations)\n",
    "\n",
    "train_chars_puntuations_vectorized = np.array(char_vectorizer(np.array(train_chars_puntuations)))\n",
    "val_chars_puntuations_vectorized = np.array(char_vectorizer(np.array(val_chars_puntuations)))\n",
    "test_chars_puntuations_vectorized = np.array(char_vectorizer(np.array(test_chars_puntuations)))\n",
    "\n",
    "one_hot_matrix = np.eye(len(char_vectorizer.get_vocabulary()))\n",
    "\n",
    "train_chars_puntuations_one_hot = one_hot_matrix[train_chars_puntuations_vectorized][:, :, 2:]\n",
    "val_chars_puntuations_one_hot = one_hot_matrix[val_chars_puntuations_vectorized][:, :, 2:]\n",
    "test_chars_puntuations_one_hot = one_hot_matrix[test_chars_puntuations_vectorized][:, :, 2:]\n",
    "train_chars_puntuations_one_hot.shape, val_chars_puntuations_one_hot.shape, test_chars_puntuations_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_count(t, val):\n",
    "    elements_equal_to_value = tf.equal(t, val)\n",
    "    as_ints = tf.cast(elements_equal_to_value, tf.int32)\n",
    "    count = tf.reduce_sum(as_ints)\n",
    "    return count\n",
    "text_vectorizer = tf.keras.layers.TextVectorization(output_sequence_length=50)\n",
    "text_vectorizer.adapt(train_sentences)\n",
    "vocab = text_vectorizer.get_vocabulary()[2:]\n",
    "\n",
    "words = tf.reshape(tf.strings.split(tf.strings.regex_replace(train_sentences, r'[!\"#$%&()\\*\\+,-\\./:;<=>?@\\[\\\\\\]^_`{|}~\\']', \"\"), ' '), shape=[-1])\n",
    "counts = {word: tf_count(words, word).numpy() for word in vocab}\n",
    "counts_df = pd.DataFrame(counts.items(), columns=[\"word\", \"count\"])\n",
    "# counts_df = counts_df.sort_values(\"count\", ascending=False)\n",
    "max_tokens = counts_df[counts_df['count'] >= 5].shape[0]\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_line_numbers_one_hot = train_data_10[\"line_number\"].to_numpy()\n",
    "val_line_numbers_one_hot = val_data_10[\"line_number\"].to_numpy()\n",
    "test_line_numbers_one_hot = test_df_100[\"line_number\"].to_numpy()\n",
    "\n",
    "train_total_lines_one_hot = train_data_10[\"total_lines\"].to_numpy()\n",
    "val_total_lines_one_hot = val_data_10[\"total_lines\"].to_numpy()\n",
    "test_total_lines_one_hot = test_df_100[\"total_lines\"].to_numpy()\n",
    "\n",
    "train_line_numbers_one_hot.shape, train_total_lines_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=max_tokens)\n",
    "train_sentences_vectorized = vectorizer.fit_transform(train_sentences).toarray()\n",
    "val_sentences_vectorized = vectorizer.transform(val_sentences).toarray()\n",
    "test_sentences_vectorized = vectorizer.transform(test_sentences).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word_char_pos_data = tf.data.Dataset.from_tensor_slices((train_sentences_vectorized, train_chars_puntuations_one_hot, train_line_numbers_one_hot, train_total_lines_one_hot))\n",
    "train_word_char_pos_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot)\n",
    "train_dataset = tf.data.Dataset.zip((train_word_char_pos_data, train_word_char_pos_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_word_char_pos_data = tf.data.Dataset.from_tensor_slices((val_sentences_vectorized, val_chars_puntuations_one_hot, val_line_numbers_one_hot, val_total_lines_one_hot))\n",
    "val_word_char_pos_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\n",
    "val_dataset = tf.data.Dataset.zip((val_word_char_pos_data, val_word_char_pos_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_word_char_pos_data = tf.data.Dataset.from_tensor_slices((test_sentences_vectorized, test_chars_puntuations_one_hot, test_line_numbers_one_hot, test_total_lines_one_hot))\n",
    "test_word_char_pos_labels = tf.data.Dataset.from_tensor_slices(test_labels_one_hot)\n",
    "test_dataset = tf.data.Dataset.zip((test_word_char_pos_data, test_word_char_pos_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "token_inputs = tf.keras.layers.Input(shape=(max_tokens,), dtype=tf.float32)\n",
    "token_x = tf.keras.layers.Dense(512, activation='relu')(token_inputs)\n",
    "token_x = tf.keras.layers.Dense(256, activation='relu')(token_x)\n",
    "token_outputs = tf.keras.layers.Dense(128, activation='relu')(token_x)\n",
    "token_model = tf.keras.Model(token_inputs, token_outputs)\n",
    "\n",
    "char_inputs = tf.keras.layers.Input(shape=(20,26,), dtype=tf.float32, name='char_puntuation_inputs')\n",
    "char_bi_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))(char_inputs)\n",
    "char_x = tf.keras.layers.Dense(256, activation='relu')(char_bi_lstm)\n",
    "char_outputs = tf.keras.layers.Dense(128, activation='relu')(char_x)\n",
    "char_model = tf.keras.Model(char_inputs, char_outputs)\n",
    "\n",
    "line_number_inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.float32, name='line_number_inputs')\n",
    "line_number_outputs = tf.keras.layers.Dense(32, activation='relu')(line_number_inputs)\n",
    "line_number_model = tf.keras.Model(line_number_inputs, line_number_outputs)\n",
    "\n",
    "total_lines_inputs = tf.keras.layers.Input(shape=(1,), name='total_lines_inputs')\n",
    "total_lines_outputs = tf.keras.layers.Dense(32, activation='relu')(total_lines_inputs)\n",
    "total_lines_model = tf.keras.Model(total_lines_inputs, total_lines_outputs)\n",
    "\n",
    "token_char_concatenate = tf.keras.layers.Concatenate(name=\"token_char_concatenate\")([token_model.output, char_model.output])\n",
    "\n",
    "drop_out = tf.keras.layers.Dense(256, activation='relu')(token_char_concatenate)\n",
    "drop_out = tf.keras.layers.Dropout(0.5)(drop_out)\n",
    "\n",
    "final_concatenate = tf.keras.layers.Concatenate(name='final_concatenate')([drop_out, line_number_model.output, total_lines_model.output])\n",
    "\n",
    "outputs = tf.keras.layers.Dense(5, activation='softmax', name='output_layer')(final_concatenate)\n",
    "\n",
    "model_m_56 = tf.keras.Model([token_model.input, char_model.input, line_number_model.input, total_lines_model.input], outputs)\n",
    "\n",
    "model_m_56.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model_m_56_history = model_m_56.fit(\n",
    "    train_dataset,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=int(len(train_dataset)),\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=int(len(val_dataset)),\n",
    "    callbacks=get_callbacks(\"model_m_56\"),\n",
    ")\n",
    "\n",
    "model_m_56.load_weights(\"skim_lit/checkpoint/model_m_56/checkpoint.ckpt\")\n",
    "\n",
    "model_m_56_val_preds = tf.argmax(model_m_56.predict(val_dataset), axis=1)\n",
    "model_m_56_val_results = calculate_results(tf.argmax(val_labels_one_hot, axis=1), model_m_56_val_preds)\n",
    "print(model_m_56_val_results)\n",
    "\n",
    "plot_loss_curves(model_m_56_history)\n",
    "\n",
    "model_m_56_preds = tf.argmax(model_m_56.predict(test_dataset), axis=1)\n",
    "model_m_56_results = calculate_results(tf.argmax(test_labels_one_hot, axis=1), model_m_56_preds)\n",
    "model_m_56_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Results of all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_val_results = pd.DataFrame({\n",
    "    \"model_m_0\": model_m_0_val_results,\n",
    "    \"model_m_1\": model_m_1_val_results,\n",
    "    \"model_m_5\": model_m_5_val_results,\n",
    "    \"model_m_23\": model_m_23_val_results,\n",
    "    \"model_m_25\": model_m_25_val_results,\n",
    "    \"model_m_32\": model_m_32_val_results,\n",
    "    \"model_m_35\": model_m_35_val_results,\n",
    "    \"model_m_37\": model_m_37_val_results,\n",
    "    \"model_m_40\": model_m_40_val_results,\n",
    "    \"model_m_43\": model_m_43_val_results,\n",
    "    \"model_m_45\": model_m_45_val_results,\n",
    "    \"model_m_51\": model_m_51_val_results,\n",
    "    \"model_m_54\": model_m_54_val_results,\n",
    "    \"model_m_55\": model_m_55_val_results,\n",
    "    \"model_m_56\": model_m_56_val_results,\n",
    "})\n",
    "\n",
    "all_val_results = all_val_results.T\n",
    "all_val_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_val_results.plot(kind=\"bar\", figsize=(10, 7)).legend(bbox_to_anchor=(1.0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_val_results[\"accuracy\"].plot(kind=\"bar\", figsize=(10, 7)).legend(bbox_to_anchor=(1.0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_val_results['f1'].plot(kind=\"bar\", figsize=(10, 7)).legend(bbox_to_anchor=(1.0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = pd.DataFrame({\n",
    "    \"model_m_0\": model_m_0_results,\n",
    "    \"model_m_1\": model_m_1_results,\n",
    "    \"model_m_5\": model_m_5_results,\n",
    "    \"model_m_23\": model_m_23_results,\n",
    "    \"model_m_25\": model_m_25_results,\n",
    "    \"model_m_32\": model_m_32_results,\n",
    "    \"model_m_35\": model_m_35_results,\n",
    "    \"model_m_37\": model_m_37_results,\n",
    "    \"model_m_40\": model_m_40_results,\n",
    "    \"model_m_43\": model_m_43_results,\n",
    "    \"model_m_45\": model_m_45_results,\n",
    "    \"model_m_51\": model_m_51_results,\n",
    "    \"model_m_54\": model_m_54_results,\n",
    "    \"model_m_55\": model_m_55_results,\n",
    "    \"model_m_56\": model_m_56_results,\n",
    "})\n",
    "\n",
    "all_results = all_results.T\n",
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results.plot(kind=\"bar\", figsize=(10, 7)).legend(bbox_to_anchor=(1.0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results[\"accuracy\"].plot(kind=\"bar\", figsize=(10, 7)).legend(bbox_to_anchor=(1.0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results['f1'].plot(kind=\"bar\", figsize=(10, 7)).legend(bbox_to_anchor=(1.0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "print(\"Notebook last run at:\")\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
